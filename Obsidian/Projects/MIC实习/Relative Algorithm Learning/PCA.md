- 思想概述：是将n维特征映射到k维上，这k维是全新的正交特征也被称为主成分。
- 原理解释：从原始的空间中顺序地找一组相互正交的坐标轴
	- 第一个新坐标轴选择是原始数据中方差最大的方向
	- 第二个新坐标轴选取是与第一个坐标轴正交的平面中使得方差最大的
	- 第三个轴是与第1,2个轴正交的平面中方差最大的
	- ……
	- 可以得到n个这样的坐标轴，大部分方差都包含在前面k个坐标轴中，后面的坐标轴所含的方差几乎为0
- 具体实现：**计算数据矩阵的协方差矩阵**，然后得到协方差矩阵的特征值特征向量，**选择特征值最大(即方差最大)的k个特征**所对应的特征向量组成的矩阵。得到协方差矩阵特征向量方法有以下两种：
	- 特征值分解协方差矩阵
	- 奇异值分解协方差矩阵
## 协方差矩阵
- 协方差矩阵是散度矩阵($XX^T$)乘上一个常数的结果，二者具有相同的特征值和特征向量，而散度矩阵是SVD分解中重要的一步。因此PCA和SVD之间存在着重要的关联
## PCA的实现
- 输入：特征集$X=\{x_1,x_2,x_3,...,x_n\}$，需要降低到k维
1. 去中心化：每一个特征减去所有特征的平均值$$X=\{x_1-m,x_2-m,x_3-m,...,x_n-m\}$$$$m=1/n\Sigma_{i=1}^nx_i$$
2. 计算协方差矩阵$1/nXX^T$
3. 求协方差矩阵特征值和特征向量（**这部分为什么使用以上形式的协方差矩阵进行分解复杂的线性代数推导**），这里也可以进行SVD，是属于不同的实现方式。
4. 对特征值从大到小排序，选择其中最大的k个。然后将其对应的k个特征向量分别作为行向量组成特征向量矩阵P。
5. 将数据转换到k个特征向量构建的新空间中，即Y=PX


