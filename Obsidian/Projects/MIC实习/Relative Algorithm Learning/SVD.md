- SVD：Singular Value Decomposition，奇异值分解[[https://mp.weixin.qq.com/s/Dv51K8JETakIKe5dPBAPVg]]
## 特征值和特征值分解
- 特征值
	- 如果一个向量v是矩阵A的特征向量，将一定可以表示成下面的形式：$$
	Av=\lambda v$$
	- 当我们求特征值与特征向量的时候，就是为了求矩阵A能使哪些向量（特征向量）只发生伸缩变换，而变换的程度可以用特征值$\lambda$表示
- 特征值的几何意义：
	- 一个矩阵实际上就是一个线性变换，一个N维的矩阵，实际上是在空间中的N个方向上对原本的坐标轴进行了不同程度的拉伸变化
- 特征值分解：
	- 特征值分解本质是将矩阵A进行如下转化$$A=Q\Sigma Q^{-1}$$
	- 这其中，Sigma是一个对角矩阵，对角线上的值就是特征值，每个特征值衡量了在对应特征向量方向上改变的幅度
	- Q就是特征向量组成的矩阵
## SVD分解
- 特征值分解的缺点：只能应用于方阵
- 奇异值分解：
	- 奇异值分解是一个能适用于任意矩阵的一种分解的方法，对于任意矩阵A总是存在一个奇异值分解：$$A=U\Sigma V^T$$
	- A是m\*n的矩阵
	- U是m\*m的方阵，V是一个n\*n的方阵，$\Sigma$是一个对角矩阵
	- U中的正交向量称为左奇异向量，$V^T$中的正交向量称为右奇异向量
	- U实际上是矩阵$AA^T$的特征向量组成的矩阵，V实际上是矩阵$A^TA$的特征向量组成的矩阵
- 奇异值分解的作用，Sigma矩阵的对角线上，奇异值从大到小的顺序减小的速度非常快，使用很少量的奇异值就可以表示A中绝大多数信息$$A_{m*n}\approx U_{m*r}\Sigma _{r*r}V_{r*n}^{T}$$
- PLSA、LSI本质上也是对矩阵进行SVD分解
- LDA：对于原数据本身就属于多种类的情况下进行降维，还是采用SVD![[Pasted image 20230214163906.png]]