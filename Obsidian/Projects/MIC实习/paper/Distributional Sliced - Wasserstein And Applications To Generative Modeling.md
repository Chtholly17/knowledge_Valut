# 概述
## 背景
- OT
- SWD和MAX-SW
## 问题
- SWD需要进行很多次不必要的投影
- MAX-SW只保留使L2距离最大的投影，放弃了很多其他有用的方向
- PWD，计算MAX-subspace Distance的投影方向，计算复杂度高，不能很好的利用一维Wasserstein Distance的性质
## 贡献
- 在参数空间标准圆（各个投影向量从上面随机采样）上计算各个投影方向的重要程度，这样MAX-SWD可以看作是在寻找最佳的(只在高维投影空间确定唯一的一个点）Delta-Dirac（除零点值为1之外其他函数值都为0的函数）分布
- 研究对投影方向的优化，在减少蒙特卡洛方法中没有意义的投影的同时，保留对Distribution信息有重要作用的投影方向
- DSW：Distributional Sliced-Wasseretein Distance，平衡了重要投影方向分布的area尺度以及投影本身的信息（也就是这个方向的投影可以区分两个target的程度）
- 将DSW应用在生成对抗模型中，表现明显优于SWD和MAX-SW
# 具体工作
- 定义：C是一个不大于1的实数，MC是参数空间中的一个集合，假如MC中满足以下式子的子集非空，那么说C是可采纳的。                        ![[Pasted image 20221201225110.png]]
- DSW的定义：![[Pasted image 20221201223609.png]]
	- sigma是一系列可能的投影方向，求sigma中所有投影方向上SWD的均值
	- 由于需要确定一系列”重要“投影分布的area，并且这个area不能是一个单独的小区域，因为这样会导致Radon-Transform后投影的数值非常接近
	- 依据以上的定义，C一定小于1（所有的投影都分布在单位圆上）M1包含了所有标准圆上的投影方向，因此在M1中优化就是寻找最佳的Dirac投影。
	- 当C很小的时候，以上的限制会导致sigma对彼此之间相隔更远（这里的更远可以理解为高维向量之间的夹角更大）的投影方向赋予更大的权重，DSW认为C更小的投影方向更重要
- 证明了DSW具有以下性质：在p>=1以及C>0的情况下
	- $DSW_p(.,.,C)$具有non-negative、symmetric、identity的性质，并且满足三角不等式
	- ![[Pasted image 20221201224931.png]]
		- 第二条性质表明SW、DSW、MAX-SW之间在特殊情况下收敛于相同的值，证明DSW是以上方式的Generalize

- DSW的计算：按照以上定义DSW难以计算，因此通常采用以下的定义进行计算![[Pasted image 20221201225311.png]]
	- p>=1,C>0，lambdaC是一个依赖于C的非负常数，M代表标准圆上所有可能的投影方向（相当于M1）
	- 不再像之前的定义一样存在MC的约束（MC可能难以计算），采用lambdaC代替实现对$E_{\theta,\theta' \textasciitilde \sigma}[|\theta^T \theta'|]$的 regularization
	- 实际上lambdaC就是这个投影方向的权重，当lambdaC很大，说明这个方向很重要（也就是C很小），当lambdaC很小，说明这个方向不重要（也就是C很大）
	- 此定义还是难以计算
- 另一种表示（定义2）：![[Pasted image 20221201231003.png]]
	- 这其中f是Borel measurable function，F是S^{d-1}上所有Borel measurable function的集合  
	baike.baidu.com/item/波莱尔可测函数/18919536
	- sigma^{d-1}是S^{d-1}上的一个均匀分布（投影方向参数空间上的均匀分布），以上式子相当于首先在投影参数空间中使用均匀分布（uniform distribution，投影参数空间是一个高维标准圆）随机取投影向量，然后使用f将它们转化为其他投影向量，并使用lambdaC进行约束，我们需要对函数f进行优化
	- 函数f的优化：使用一个神经网络表示函数f，其参数为pi，将其记作f_pi，使用上述式子中的DS(f)进行梯度下降以优化函数f，也就是找上述式子的最大值
	- 在Uniform Distribution上有多种对DS(f)梯度的预测（可能有多个梯度方向？），采用蒙特卡洛方式进行处理
	- lambdaC作为一个正则化参数，在各个实际应用中需要进行fine tune
- 类似于GSW，将DSW扩充为DGSW
# 实验
# 定性实验
- lambdaC从1、10、100、1000中取值，取使随机10000张生成的图片和所有validation set中的图片中W2距离的最小者
- 定义2中lambdaC和C的作用：
	- 在两个高斯分布下的结果![[Pasted image 20221201233257.png]]
		- 以上红点表示对需要计算的投影空间sigma的可视化
		- 采用两个高斯分布，均值均为0，具有不同的协方差矩阵
		- lambdaC等于0，distribution中仅关注一个方向（相当于M1情况，如果是最优化狄拉克函数就等同于求MAX-SW），此方向可能无法对两个分布进行有效的区分
		- lambdaC越大，对应的C越小，circle上其他方向获得了更多的权重
		- LambdaC等于1000，非常大的时候（对应的C应该比较小），参数空间上的分布更关注两个特征向量，也就是区分两个高斯分布的特征向量 
	- 在MINST上的结果                                             ![[Pasted image 20221202140738.png]]
	- 可以发现，随着lambdaC的增大，在两个高斯分布上的空间之中任意两个向量夹角的积分结果并没有在MINST数据集上的结果小。这可能说明了越是具有结构化信息的真实图片，这种约束方式更能在向量空间中找到能区分去Distribution Difference的投影方向
## 定量实验
- 生成图片质量![[Pasted image 20221202140622.png]]
	- 可以看到DSW在相同的运算时间和迭代次数（可能是指投影的次数）下均表现更好，并且DSW在比较小的迭代次数下就可以很好地收敛
	- 有些情况下PSW略微好于DSW，但代价是远大于DSW的计算时间
- 计算速度                                            ![[Pasted image 20221202141203.png]]
	- 可以看到DSW和SW表现差不多，优于MAX-SW
- 在大规模数据集上的测试：使用FIR指标，另一篇论文中提到的评价生成质量的指标，越低越好![[Pasted image 20221202142155.png]]
	- DSW在大规模数据集上无法和PSW比较，PSW以高昂的训练时间换取了更好的生成效果
	- 和SW、MAX-SW等比较，在相同条件下，DSW有很大的提升
## 启发
- 主要的思想也就是采用方向差距更大的投影向量，以获取更多分布上的不同。实现上使用了一个网络，使用梯度下降的方式寻找更能表现出更大Distribution Difference 的投影方向，可以应用在我们的任务中。
- 虽然进行了训练，但是这个训练过程是与具体的图片无关的，关注的仅仅是参数空间之间的距离。如果考虑到具体问题中的具体图片，是否可以在投影结束之后的一维空间内计算这个距离，使用梯度下降求极大值。当然如果对每张图片都这样操作还不如使用蒙特卡洛方法进行逼近，但现实数据集中很多图片往往具有相似的结构特征，是否可以用有代表性的图片先进行训练，使用这样预训练的网络来进行投影方向的确定呢？
- 考虑到PSW在大规模数据集上以更高的时间代价换取了更好的效果，我们也可以权衡考虑使用PSW来计算我们的LOSS