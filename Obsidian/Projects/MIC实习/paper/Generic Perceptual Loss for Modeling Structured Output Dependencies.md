## 背景
-  perceptual loss：为什么有效？![[Pasted image 20230325152303.png]]
- dense prediction tasks：semantic segmentation、depth estimation、object detection
## 问题
- 现在的观点认为perceptual loss有效的主要原因在于**使用大数据集预训练的CNN模型可以有效地捕捉到图像中高层语义信息
## 贡献
- 本文研究认为：
	- **特征提取网络的结构**才是影响perceptual loss效果的关键因素
	- **the effect of perceptual loss does not depend on pretrained CNN weights**，经过训练的多层CNN已经有能力捕获图像中的多层级信息
	- SR任务中，random VGG和pretrained VGG都获得了几乎相同的结果![[Pasted image 20230325142146.png]]
## 具体工作
- 在一系列dense prediction 任务上应用了perceptual loss，获取了不错的结果
- 使用随机VGG 作为feature extractor，**不恰当的随机初始化方式可能导致不稳定的结果
	- **It is important to guarantee that each layer is bounded by a Lipschitz constant close to 1, so that the gradient generated by the random network will not explode or vanish
	- 这篇文章中提出的方法可以避免梯度爆炸或者消失，以后进行消融实验的时候注意一下[[Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiﬁers: Surpassing human-level performance on imagenet classiﬁcation. In Proc. IEEE Int. Conf. Comp. Vis., pages 1026–1034, 2015.]]
## 实验
- Training Weights vs. Architecture
	- 定性结果：发现是否训练效果差不多，同时VGG一系列的网络表现得比其他网络结构好![[Pasted image 20230325151312.png]]
- Design of the Perceptual Loss Network
	- Impact of depth.：每个block只有一个卷积层，性能也不错，速度还更快，说明depth不是主要因素![[Pasted image 20230325152500.png]]
	- Impact of the receptive ﬁeld：感受野的大小好像有点关系，感受野越大似乎性能更加好![[Pasted image 20230325152807.png]]
	- Impact of multi-level losses：Combining multi-level of the losses may lead to slightly better accuracy with more hyper-parameters.（different weights）
		- 只使用最后一层的feature，效果也不错
## 启发
- 这篇文章中提到的参数不会影响效果的例子，很可能是仔细挑选过后的任务，没有任何理论层面的依据
- 根据[[Understanding and Simplifying Perceptual Distances]]中的实验：对于随机VGG，当两张图片之间的差异较大时，就会出现效果不好的情况，现在有这种想法
	- 相似的图片本身就具有相同的分布，这样经过相同的随机变化之后，计算其之间的距离也是很能反应原来的高级信息差距的
	- 不同的图片由于分布差别较大，经过相同的随机变化可能会破坏其原本在高级信息方面的差别
	- 现在的关键在于：**训练的过程中发生了什么，让这个变化变得有效了呢？