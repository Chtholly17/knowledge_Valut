# 实验背景
本次CV作业，我的实验对象为AIGC模型。AIGC全称为Artificial Intelligence Generate Content，作用是由人工智能模型自动生成包括图片、文字等内容，现在大火的chatgpt就属于text-to-text的生成模型，其性能表现已经给大家留下了深刻的印象。
实际上，生成模型不仅可以生成文字信息，还可以生成图片，其中的原理就包含了很多计算机视觉相关的专业知识。比如，当下同样备受关注的人工智能绘画技术就应用到了text-to-image生成模型。本质上，无论是生成图片还是生成文字，其关键都是在于如何对输入的信息进行编码，将其转化为高维空间容易处理的向量，然后对这个向量进行某种计算，这些计算中，往往会为向量上添加某些我们希望的信息，然后使用解码器将其投射回图像或者文字空间，完成生成。
可见，chatgpt这类语言模型，实际上与计算机视觉领域也存在着千丝万缕的联系。最早用于NLP领域的transformer模型，应用在很多图像处理任务上也可以获得非常优秀的效果，更加说明了这一点。因此，本次实验我就选择了其中和CV关系最为紧密的图像生成模型作为我的实验对象。
当前效果最好的图像生成模型分别是stable diffusion模型、openai发布的Dall-e模型，我首先尝试了可以直接通过调用api进行实验的Dall-e模型，但发现openai对于api的调用存在严格的限制，国内难以进行实验。因此我选择了效果同样优秀的stable diffusion模型，将其部署在我自己租用的服务器上进行实验。

# 实验过程

## 模型下载和环境配置

首先，我在服务器上clone了stable diffusion项目的源码，并使用进行了相关的环境配置![[Pasted image 20230314221714.png]]
接下来，我前往hugging face网站中下载到了stable-duffsion v1.4的参数，并且将其存放在对应的路径中![[Pasted image 20230315005246.png]]
![[Pasted image 20230315005353.png]]
接下来，我使用如下的测试程序进行测试：
```python
import argparse, os, sys, glob
import cv2
import torch
import numpy as np
from omegaconf import OmegaConf
from PIL import Image
from tqdm import tqdm, trange
from imwatermark import WatermarkEncoder
from itertools import islice
from einops import rearrange
from torchvision.utils import make_grid
import time
from pytorch_lightning import seed_everything
from torch import autocast
from contextlib import contextmanager, nullcontext

from diffusers.pipelines.stable_diffusion.safety_checker import StableDiffusionSafetyChecker
from transformers import AutoFeatureExtractor


from ldm.util import instantiate_from_config
from ldm.models.diffusion.ddim import DDIMSampler
from ldm.models.diffusion.plms import PLMSSampler
from ldm.models.diffusion.dpm_solver import DPMSolverSampler


# load safety model
safety_model_id = "CompVis/stable-diffusion-safety-checker"
safety_feature_extractor = AutoFeatureExtractor.from_pretrained(safety_model_id)
safety_checker = StableDiffusionSafetyChecker.from_pretrained(safety_model_id)


def chunk(it, size):
    it = iter(it)
    return iter(lambda: tuple(islice(it, size)), ())


def numpy_to_pil(images):
    """
    Convert a numpy image or a batch of images to a PIL image.
    """
    if images.ndim == 3:
        images = images[None, ...]
    images = (images * 255).round().astype("uint8")
    pil_images = [Image.fromarray(image) for image in images]

    return pil_images


def load_model_from_config(config, ckpt, verbose=False):
    print(f"Loading model from {ckpt}")
    pl_sd = torch.load(ckpt, map_location="cpu")
    if "global_step" in pl_sd:
        print(f"Global Step: {pl_sd['global_step']}")
    sd = pl_sd["state_dict"]
    model = instantiate_from_config(config.model)
    m, u = model.load_state_dict(sd, strict=False)
    if len(m) > 0 and verbose:
        print("missing keys:")
        print(m)
    if len(u) > 0 and verbose:
        print("unexpected keys:")
        print(u)

    model.cuda()
    model.eval()
    return model


def put_watermark(img, wm_encoder=None):
    if wm_encoder is not None:
        img = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)
        img = wm_encoder.encode(img, 'dwtDct')
        img = Image.fromarray(img[:, :, ::-1])
    return img


def load_replacement(x):
    try:
        hwc = x.shape
        y = Image.open("assets/rick.jpeg").convert("RGB").resize((hwc[1], hwc[0]))
        y = (np.array(y)/255.0).astype(x.dtype)
        assert y.shape == x.shape
        return y
    except Exception:
        return x


def check_safety(x_image):
    safety_checker_input = safety_feature_extractor(numpy_to_pil(x_image), return_tensors="pt")
    x_checked_image, has_nsfw_concept = safety_checker(images=x_image, clip_input=safety_checker_input.pixel_values)
    assert x_checked_image.shape[0] == len(has_nsfw_concept)
    for i in range(len(has_nsfw_concept)):
        if has_nsfw_concept[i]:
            x_checked_image[i] = load_replacement(x_checked_image[i])
    return x_checked_image, has_nsfw_concept


def main():
    parser = argparse.ArgumentParser()

    parser.add_argument(
        "--prompt",
        type=str,
        nargs="?",
        default="a painting of a virus monster playing guitar",
        help="the prompt to render"
    )
    parser.add_argument(
        "--outdir",
        type=str,
        nargs="?",
        help="dir to write results to",
        default="outputs/txt2img-samples"
    )
    parser.add_argument(
        "--skip_grid",
        action='store_true',
        help="do not save a grid, only individual samples. Helpful when evaluating lots of samples",
    )
    parser.add_argument(
        "--skip_save",
        action='store_true',
        help="do not save individual samples. For speed measurements.",
    )
    parser.add_argument(
        "--ddim_steps",
        type=int,
        default=50,
        help="number of ddim sampling steps",
    )
    parser.add_argument(
        "--plms",
        action='store_true',
        help="use plms sampling",
    )
    parser.add_argument(
        "--dpm_solver",
        action='store_true',
        help="use dpm_solver sampling",
    )
    parser.add_argument(
        "--laion400m",
        action='store_true',
        help="uses the LAION400M model",
    )
    parser.add_argument(
        "--fixed_code",
        action='store_true',
        help="if enabled, uses the same starting code across samples ",
    )
    parser.add_argument(
        "--ddim_eta",
        type=float,
        default=0.0,
        help="ddim eta (eta=0.0 corresponds to deterministic sampling",
    )
    parser.add_argument(
        "--n_iter",
        type=int,
        default=2,
        help="sample this often",
    )
    parser.add_argument(
        "--H",
        type=int,
        default=512,
        help="image height, in pixel space",
    )
    parser.add_argument(
        "--W",
        type=int,
        default=512,
        help="image width, in pixel space",
    )
    parser.add_argument(
        "--C",
        type=int,
        default=4,
        help="latent channels",
    )
    parser.add_argument(
        "--f",
        type=int,
        default=8,
        help="downsampling factor",
    )
    parser.add_argument(
        "--n_samples",
        type=int,
        default=3,
        help="how many samples to produce for each given prompt. A.k.a. batch size",
    )
    parser.add_argument(
        "--n_rows",
        type=int,
        default=0,
        help="rows in the grid (default: n_samples)",
    )
    parser.add_argument(
        "--scale",
        type=float,
        default=7.5,
        help="unconditional guidance scale: eps = eps(x, empty) + scale * (eps(x, cond) - eps(x, empty))",
    )
    parser.add_argument(
        "--from-file",
        type=str,
        help="if specified, load prompts from this file",
    )
    parser.add_argument(
        "--config",
        type=str,
        default="configs/stable-diffusion/v1-inference.yaml",
        help="path to config which constructs model",
    )
    parser.add_argument(
        "--ckpt",
        type=str,
        default="models/ldm/stable-diffusion-v1/model.ckpt",
        help="path to checkpoint of model",
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=42,
        help="the seed (for reproducible sampling)",
    )
    parser.add_argument(
        "--precision",
        type=str,
        help="evaluate at this precision",
        choices=["full", "autocast"],
        default="autocast"
    )
    opt = parser.parse_args()

    if opt.laion400m:
        print("Falling back to LAION 400M model...")
        opt.config = "configs/latent-diffusion/txt2img-1p4B-eval.yaml"
        opt.ckpt = "models/ldm/text2img-large/model.ckpt"
        opt.outdir = "outputs/txt2img-samples-laion400m"

    seed_everything(opt.seed)

    config = OmegaConf.load(f"{opt.config}")
    model = load_model_from_config(config, f"{opt.ckpt}")

    device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
    model = model.to(device)

    if opt.dpm_solver:
        sampler = DPMSolverSampler(model)
    elif opt.plms:
        sampler = PLMSSampler(model)
    else:
        sampler = DDIMSampler(model)

    os.makedirs(opt.outdir, exist_ok=True)
    outpath = opt.outdir

    print("Creating invisible watermark encoder (see https://github.com/ShieldMnt/invisible-watermark)...")
    wm = "StableDiffusionV1"
    wm_encoder = WatermarkEncoder()
    wm_encoder.set_watermark('bytes', wm.encode('utf-8'))

    batch_size = opt.n_samples
    n_rows = opt.n_rows if opt.n_rows > 0 else batch_size
    if not opt.from_file:
        prompt = opt.prompt
        assert prompt is not None
        data = [batch_size * [prompt]]

    else:
        print(f"reading prompts from {opt.from_file}")
        with open(opt.from_file, "r") as f:
            data = f.read().splitlines()
            data = list(chunk(data, batch_size))

    sample_path = os.path.join(outpath, "samples")
    os.makedirs(sample_path, exist_ok=True)
    base_count = len(os.listdir(sample_path))
    grid_count = len(os.listdir(outpath)) - 1

    start_code = None
    if opt.fixed_code:
        start_code = torch.randn([opt.n_samples, opt.C, opt.H // opt.f, opt.W // opt.f], device=device)

    precision_scope = autocast if opt.precision=="autocast" else nullcontext
    with torch.no_grad():
        with precision_scope("cuda"):
            with model.ema_scope():
                tic = time.time()
                all_samples = list()
                for n in trange(opt.n_iter, desc="Sampling"):
                    for prompts in tqdm(data, desc="data"):
                        uc = None
                        if opt.scale != 1.0:
                            uc = model.get_learned_conditioning(batch_size * [""])
                        if isinstance(prompts, tuple):
                            prompts = list(prompts)
                        c = model.get_learned_conditioning(prompts)
                        shape = [opt.C, opt.H // opt.f, opt.W // opt.f]
                        samples_ddim, _ = sampler.sample(S=opt.ddim_steps,
                                                         conditioning=c,
                                                         batch_size=opt.n_samples,
                                                         shape=shape,
                                                         verbose=False,
                                                         unconditional_guidance_scale=opt.scale,
                                                         unconditional_conditioning=uc,
                                                         eta=opt.ddim_eta,
                                                         x_T=start_code)

                        x_samples_ddim = model.decode_first_stage(samples_ddim)
                        x_samples_ddim = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)
                        x_samples_ddim = x_samples_ddim.cpu().permute(0, 2, 3, 1).numpy()

                        # x_checked_image, has_nsfw_concept = check_safety(x_samples_ddim)
                        x_checked_image = x_samples_ddim

                        x_checked_image_torch = torch.from_numpy(x_checked_image).permute(0, 3, 1, 2)

                        if not opt.skip_save:
                            for x_sample in x_checked_image_torch:
                                x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')
                                img = Image.fromarray(x_sample.astype(np.uint8))
                                img = put_watermark(img, wm_encoder)
                                img.save(os.path.join(sample_path, f"{base_count:05}.png"))
                                base_count += 1

                        if not opt.skip_grid:
                            all_samples.append(x_checked_image_torch)

                if not opt.skip_grid:
                    # additionally, save as grid
                    grid = torch.stack(all_samples, 0)
                    grid = rearrange(grid, 'n b c h w -> (n b) c h w')
                    grid = make_grid(grid, nrow=n_rows)

                    # to image
                    grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()
                    img = Image.fromarray(grid.astype(np.uint8))
                    img = put_watermark(img, wm_encoder)
                    img.save(os.path.join(outpath, f'grid-{grid_count:04}.png'))
                    grid_count += 1

                toc = time.time()

if __name__ == "__main__":
    main()

```

## bug修复

接下来，我输入运行测试程序的指令，要求模型生成一个骑老虎的宇航员
```sh
python test.py --prompt "a photograph of an astronaut riding a tiger" --plms
```
出现如下报错![[Pasted image 20230315004835.png]]
原因在于作者对于taming中库的名称进行了修改，我在github的issue中查询到了相关问题，并且进行了对应的修改![[Pasted image 20230315005014.png]]
![[Pasted image 20230315005028.png]]

## image-to-image生成结果

在进行上述的修改之后，测试程序成功运行，以下为生成的多张图片：![[grid-0003.png]]
可以发现，由于早期model迭代次数较少，并且数据集中的图片不够丰富，导致网络生成的效果其实并不总是尽如人意，很多图片的content并不符合我们的要求。（在进行生成宇航员骑老虎图片的同时，我还进行了一些生成人物图片的测试，效果也不总是很理想，有些生成结果人体结构发生畸变，非常恐怖，在此就不进行展示了）
其中，content比较符合我们描述的应该是如下的图片![[Pasted image 20230315234148.png]]但是这张图片属于偏向卡通风格的。而我想要的是风格比较真实图片，类似于这一张![[Pasted image 20230315235553.png]]
但这张图片的content存在一些问题，不符合我们宇航员骑老虎的要求。可见text-to-image只可以控制生成图像的内容，但是无法控制生成图像的风格（style），如果对style不满意只能重新生成，而这又无法保证具有我们喜欢的style的图片的content还符合我们的要求。
之后我重新进行了生成实验，就算在生成时加上了对风格的限制，模型也可能无法满足我们对风格的要求
```shell
python test.py --prompt "a photograph of an astronaut riding a tiger, the generated photograph should look real" --plms
```
以下是加上风格要求后生成的图片，和原本生成的图片有同样的问题![[grid-0005.png]]

## text-to-image和image-to-image的结合

此时我注意到，stable Diffusion模型不仅可以应用在text-to-image任务上，还可以应用在image-to-image任务中，实现类似风格迁移的效果，这需要我们输入一张图片，然后用自然语言描绘我们需要想要的风格，模型可以在保留原本图片content的基础上生成拥有我们想要风格的图片，那我是否可以将image-to-image作用在text-to-image生成的图片上，然后得到既具有我们要求的content，又具有我们想要的style的图片呢？
基于以上想法，我采用了如下的新测试脚本，以上一次我们生成的宇航员骑老虎图片作为输入，进行了新的实验
```python
"""make variations of input image"""

import argparse, os, sys, glob
import PIL
import torch
import numpy as np
from omegaconf import OmegaConf
from PIL import Image
from tqdm import tqdm, trange
from itertools import islice
from einops import rearrange, repeat
from torchvision.utils import make_grid
from torch import autocast
from contextlib import nullcontext
import time
from pytorch_lightning import seed_everything

from ldm.util import instantiate_from_config
from ldm.models.diffusion.ddim import DDIMSampler
from ldm.models.diffusion.plms import PLMSSampler


def chunk(it, size):
    it = iter(it)
    return iter(lambda: tuple(islice(it, size)), ())


def load_model_from_config(config, ckpt, verbose=False):
    print(f"Loading model from {ckpt}")
    pl_sd = torch.load(ckpt, map_location="cpu")
    if "global_step" in pl_sd:
        print(f"Global Step: {pl_sd['global_step']}")
    sd = pl_sd["state_dict"]
    model = instantiate_from_config(config.model)
    m, u = model.load_state_dict(sd, strict=False)
    if len(m) > 0 and verbose:
        print("missing keys:")
        print(m)
    if len(u) > 0 and verbose:
        print("unexpected keys:")
        print(u)

    model.cuda()
    model.eval()
    return model


def load_img(path):
    image = Image.open(path).convert("RGB")
    w, h = image.size
    print(f"loaded input image of size ({w}, {h}) from {path}")
    w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32
    image = image.resize((w, h), resample=PIL.Image.LANCZOS)
    image = np.array(image).astype(np.float32) / 255.0
    image = image[None].transpose(0, 3, 1, 2)
    image = torch.from_numpy(image)
    return 2.*image - 1.


def main():
    parser = argparse.ArgumentParser()

    parser.add_argument(
        "--prompt",
        type=str,
        nargs="?",
        default="a painting of a virus monster playing guitar",
        help="the prompt to render"
    )

    parser.add_argument(
        "--init-img",
        type=str,
        nargs="?",
        help="path to the input image"
    )

    parser.add_argument(
        "--outdir",
        type=str,
        nargs="?",
        help="dir to write results to",
        default="outputs/img2img-samples"
    )

    parser.add_argument(
        "--skip_grid",
        action='store_true',
        help="do not save a grid, only individual samples. Helpful when evaluating lots of samples",
    )

    parser.add_argument(
        "--skip_save",
        action='store_true',
        help="do not save indiviual samples. For speed measurements.",
    )

    parser.add_argument(
        "--ddim_steps",
        type=int,
        default=50,
        help="number of ddim sampling steps",
    )

    parser.add_argument(
        "--plms",
        action='store_true',
        help="use plms sampling",
    )
    parser.add_argument(
        "--fixed_code",
        action='store_true',
        help="if enabled, uses the same starting code across all samples ",
    )

    parser.add_argument(
        "--ddim_eta",
        type=float,
        default=0.0,
        help="ddim eta (eta=0.0 corresponds to deterministic sampling",
    )
    parser.add_argument(
        "--n_iter",
        type=int,
        default=1,
        help="sample this often",
    )
    parser.add_argument(
        "--C",
        type=int,
        default=4,
        help="latent channels",
    )
    parser.add_argument(
        "--f",
        type=int,
        default=8,
        help="downsampling factor, most often 8 or 16",
    )
    parser.add_argument(
        "--n_samples",
        type=int,
        default=2,
        help="how many samples to produce for each given prompt. A.k.a batch size",
    )
    parser.add_argument(
        "--n_rows",
        type=int,
        default=0,
        help="rows in the grid (default: n_samples)",
    )
    parser.add_argument(
        "--scale",
        type=float,
        default=5.0,
        help="unconditional guidance scale: eps = eps(x, empty) + scale * (eps(x, cond) - eps(x, empty))",
    )

    parser.add_argument(
        "--strength",
        type=float,
        default=0.75,
        help="strength for noising/unnoising. 1.0 corresponds to full destruction of information in init image",
    )
    parser.add_argument(
        "--from-file",
        type=str,
        help="if specified, load prompts from this file",
    )
    parser.add_argument(
        "--config",
        type=str,
        default="configs/stable-diffusion/v1-inference.yaml",
        help="path to config which constructs model",
    )
    parser.add_argument(
        "--ckpt",
        type=str,
        default="models/ldm/stable-diffusion-v1/model.ckpt",
        help="path to checkpoint of model",
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=42,
        help="the seed (for reproducible sampling)",
    )
    parser.add_argument(
        "--precision",
        type=str,
        help="evaluate at this precision",
        choices=["full", "autocast"],
        default="autocast"
    )

    opt = parser.parse_args()
    seed_everything(opt.seed)

    config = OmegaConf.load(f"{opt.config}")
    model = load_model_from_config(config, f"{opt.ckpt}")

    device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
    model = model.to(device)

    if opt.plms:
        raise NotImplementedError("PLMS sampler not (yet) supported")
        sampler = PLMSSampler(model)
    else:
        sampler = DDIMSampler(model)

    os.makedirs(opt.outdir, exist_ok=True)
    outpath = opt.outdir

    batch_size = opt.n_samples
    n_rows = opt.n_rows if opt.n_rows > 0 else batch_size
    if not opt.from_file:
        prompt = opt.prompt
        assert prompt is not None
        data = [batch_size * [prompt]]

    else:
        print(f"reading prompts from {opt.from_file}")
        with open(opt.from_file, "r") as f:
            data = f.read().splitlines()
            data = list(chunk(data, batch_size))

    sample_path = os.path.join(outpath, "samples")
    os.makedirs(sample_path, exist_ok=True)
    base_count = len(os.listdir(sample_path))
    grid_count = len(os.listdir(outpath)) - 1

    assert os.path.isfile(opt.init_img)
    init_image = load_img(opt.init_img).to(device)
    init_image = repeat(init_image, '1 ... -> b ...', b=batch_size)
    init_latent = model.get_first_stage_encoding(model.encode_first_stage(init_image))  # move to latent space

    sampler.make_schedule(ddim_num_steps=opt.ddim_steps, ddim_eta=opt.ddim_eta, verbose=False)

    assert 0. <= opt.strength <= 1., 'can only work with strength in [0.0, 1.0]'
    t_enc = int(opt.strength * opt.ddim_steps)
    print(f"target t_enc is {t_enc} steps")

    precision_scope = autocast if opt.precision == "autocast" else nullcontext
    with torch.no_grad():
        with precision_scope("cuda"):
            with model.ema_scope():
                tic = time.time()
                all_samples = list()
                for n in trange(opt.n_iter, desc="Sampling"):
                    for prompts in tqdm(data, desc="data"):
                        uc = None
                        if opt.scale != 1.0:
                            uc = model.get_learned_conditioning(batch_size * [""])
                        if isinstance(prompts, tuple):
                            prompts = list(prompts)
                        c = model.get_learned_conditioning(prompts)

                        # encode (scaled latent)
                        z_enc = sampler.stochastic_encode(init_latent, torch.tensor([t_enc]*batch_size).to(device))
                        # decode it
                        samples = sampler.decode(z_enc, c, t_enc, unconditional_guidance_scale=opt.scale,
                                                 unconditional_conditioning=uc,)

                        x_samples = model.decode_first_stage(samples)
                        x_samples = torch.clamp((x_samples + 1.0) / 2.0, min=0.0, max=1.0)

                        if not opt.skip_save:
                            for x_sample in x_samples:
                                x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')
                                Image.fromarray(x_sample.astype(np.uint8)).save(
                                    os.path.join(sample_path, f"{base_count:05}.png"))
                                base_count += 1
                        all_samples.append(x_samples)

                if not opt.skip_grid:
                    # additionally, save as grid
                    grid = torch.stack(all_samples, 0)
                    grid = rearrange(grid, 'n b c h w -> (n b) c h w')
                    grid = make_grid(grid, nrow=n_rows)

                    # to image
                    grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()
                    Image.fromarray(grid.astype(np.uint8)).save(os.path.join(outpath, f'grid-{grid_count:04}.png'))
                    grid_count += 1

                toc = time.time()

if __name__ == "__main__":
    main()
```
我首先使用以下的指令进行了测试，控制模型将图像风格变得更加真实：
```shell
python img2img.py --prompt "A photo looks realistic." --init-img result.png --strength 0.8
```
生成的结果如下：![[Pasted image 20230316003051.png]]
结果并不令人满意，模型并没有很好地保留原图的content，宇航员变成了奇怪的无面男人，老虎也变成了颜色相近的马。我想到，是否可以在image-to-image转化的过程中，在style的描述中添加原本图片content的描述，这样也许可以更好地帮助模型理解输入图像的内容。
基于以上想法，我使用新的指令进行了实验
```python
python img2img.py --prompt "A photo looks realistic. The input image is an astronaut riding a tiger" --init-img result.png --strength 0.8
```
生成的图片如下，基本在保留了输入图片content的前提下，使图像风格更加真实了![[Pasted image 20230316003510.png]]

## 模型的可视化工作

由于本次实验，模型被部署在服务器端，而运行Linux ubuntu操作系统的服务器并没有提供可视化界面，因此如何可视化地展示模型的运行结果成为了难题。
十分幸运，在探索过程中我发现了由Meta公司开发的Visdom可视化工具[[https://ai.facebook.com/tools/visdom/]]，该工具专门用于深度学习实验的可视化，可以获取模型产生的tensor或者图片，并且以webui的形式展示出来。我们只需要以特定的端口号访问服务器，就可以实现模型的可视化工作

- 初步尝试连接
我使用的服务器租赁平台提供了自定义服务的方法，我需要在服务器上启动visdom并将其端口设置为6006，就可以提供平台提供的url访问visdom的可视化界面，只需要输入如下的指令：
```shell
visdom -port 6006
```
然而，输入指令后出现了如下的报错：![[Pasted image 20230318203359.png]]
在晚上查询相关issue之后，发现应该是网络问题，联系到国内的网络环境，出现这样的问题非常正常，我在网上查询了相关资料，在服务器上配置了学术资源加速器（配了个梯子），使用梯子重新下载脚本，最后解决了这个问题![[Pasted image 20230318204821.png]]
如下为成功打开的空白visdom界面![[Pasted image 20230318204903.png]]

- 尝试显示一张图片
首先，我尝试在visdom提供的webui中显示一张或多张随机的图片，我采用了如下的python 代码进行测试
```python
from visdom import Visdom
import numpy as np
import math
import os.path
import getpass
from sys import platform as _platform
from six.moves import urllib

viz = Visdom()
assert viz.check_connection()

#单张
viz.image(
np.random.rand(3, 512, 256),
opts=dict(title='Random!', caption='How random.'),
)

#多张
viz.images(
np.random.randn(20, 3, 64, 64),
opts=dict(title='Random images', caption='How random.')
)
```
出现如下报错：![[Pasted image 20230318210010.png]]
查询相关资料后，我发现原因可能是由于visdom以8097端口进行连接，而平台需要使用6006端口，我也是使用6006端口启动的webui。因此我在初始化visdom时设置端口为6006，解决了此问题：
```python
viz = Visdom(port= 6006)
```
webui中也成功显示了我生成的随机图片![[Pasted image 20230318210239.png]]

- 添加交互
由于近期生成模型的火爆，github上存在不少现有的基于visdom和webui，我在服务器上配置好了visdom的环境之后，就去寻找了一份拥有现有的webui用于模型的可视化。
首先在github上找到对应的项目[[https://github.com/AUTOMATIC1111/stable-diffusion-webui]]，并将项目克隆到我的服务器上：![[Pasted image 20230318222236.png]]
接下来，运行已经编辑完成的webui，运行效果如下：![[Pasted image 20230318230721.png]]

# 实验总结

本次实验，我对自己感兴趣的AIGC模型Stable Diffusion进行了测试，发现早期模型参数与现在的AI绘画模型在生成表现上还是存在一定差距。虽然据我了解，现在很多效果非常好的AI绘画软件采用的也是Stable Diffusion模型，但可能他们采用的模型参数进行了更多的训练，输入了更多的图像，或者进行了针对性的fine-tune，因此效果表现明显好于我这次的实验结果。
同时，我针对可视化工具visdom进行了探索，使用现有项目提供的可视化包对stable diffusion模型进行了可视化，可以使用webui输入生成需求并生成对应的图片。但由于国内网络环境存在一定问题，本实验成功概率较低，需要在网络环境较好的时候才有概率成功。不过本次实验还是让我学习了visdom这一工具的使用，以后的实验中可以考虑使用这一可视化工具动态地生成loss折线图而更好地观察训练效果。
本次实验是我对生成模型的第一次探索性实验，与我之前研究的CV方向不同。通过本次的实验，我对生成模型有了初步的认识，在日后的研究和学习中，我希望能更加深入地对生成模型进行探索。