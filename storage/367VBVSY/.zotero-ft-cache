Pooling by Sliced-Wasserstein Embedding
Navid Naderializadehâˆ— Department of Electrical and Systems Engineering
University of Pennsylvania Philadelphia, PA 19104
nnaderi@seas.upenn.edu
Joseph F. Comer, Reed W. Andrews, Heiko Hoffmann HRL Laboratories, LLC. Malibu, CA 90265
{jfcomer, rwandrews, hhoffmann}@hrl.com
Soheil Kolouriâˆ— Computer Science Department
Vanderbilt University Nashville, TN 37235 soheil.kolouri@vanderbilt.edu
Abstract
Learning representations from sets has become increasingly important with many applications in point cloud processing, graph learning, image/video recognition, and object detection. We introduce a geometrically-interpretable and generic pooling mechanism for aggregating a set of features into a ï¬xed-dimensional representation. In particular, we treat elements of a set as samples from a probability distribution and propose an end-to-end trainable Euclidean embedding for sliced-Wasserstein distance to learn from set-structured data effectively. We evaluate our proposed pooling method on a wide variety of set-structured data, including point-cloud, graph, and image classiï¬cation tasks, and demonstrate that our proposed method provides superior performance over existing set representation learning approaches. Our code is available at https://github.com/navid-naderi/PSWE.
1 Introduction
Many modern machine learning (ML) tasks deal with learning from set-structured data. In some cases, the input object itself is a set, as in point cloud classiï¬cation/regression, and in other cases, the complex input object is described as a set of features after being processed through a backbone, i.e., a feature extractor. For instance, in graph mining, a graph is represented as a set of node embeddings, and in computer vision, an image is represented as a set of local features extracted from its different regions (i.e., ï¬elds of view). There are unique challenges in dealing with such set-structured data, namely: i) the set cardinalities could differ from one instance to another, and ii) the elements of the set do not necessarily have an inherent ordering. These challenges call for ML models that can both handle varied input sizes and are invariant to permutations, i.e., the model output does not change under any permutation of the input set elements. Prior work on learning from set-structured data can be broadly categorized as methods based on either implicit or explicit embedding of sets into a Hilbert space. Implicit embedding approaches
âˆ—Work done while at HRL Laboratories, LLC.
35th Conference on Neural Information Processing Systems (NeurIPS 2021).

(i.e., kernel methods) rely on deï¬ning a distance/similarity measure (i.e., a kernel) between two sets [1, 2, 3, 4, 5, 6, 7, 8, 9]. These methods involve one of the two strategies of 1) solving a correspondence problem between elements of the input sets and measuring the similarity between corresponding elements, or 2) comparing all pairs of elements between the two sets based on a similarity measure (e.g., approaches based on Maximum Mean Discrepancy). On the other hand, explicit embedding methods learn a permutation-invariant mapping into a Hilbert space and provide a ï¬xed-dimensional representation for a given input set that classic ML approaches could further process [10, 11, 12]. More recently, algorithms based on a composition of permutation-equivariant neural network backbones and permutation-invariant pooling mechanisms have been proposed to deï¬ne a parametric permutation-invariant mapping [11, 13, 14, 15, 12, 16]. Notably, Zaheer et al. [11] proved that such a composition provides a universal approximator for any set function. Lee et al. [14] further showed that utilizing permutation-equivariant backbones that do not process set elements independently but model the interactions between the set elements (e.g., using self-attention) is theoretically and numerically advantageous. Similar observations have been made in the ï¬eld of graph learning using various graph neural network (GNN) architectures [17, 18, 19]. In parallel, several works have studied the importance of permutation-invariant pooling mechanisms to go beyond the commonly used mean, sum, max, or similar operators [12, 13, 15, 16].

Setä¸­çš„å…ƒç´ é‡‡â½¤

A convenient interpretation in dealing with sets is considering their elements as samples from an unknown underlying probability distribution and comparing/embedding these probability distributions

äºâ¼€ä¸ªä¸çŸ¥é“ to perform set learning. Due to this interpretation, optimal transport has played a prominent role in

distributionçš„ä¿¡ learning from sets. For instance, Kusner et al. [20] and later Huang et al. [21] represented a document

å·ï¼Œéœ€è¦â½è¾ƒå¯ as a set of words. They leveraged the 1-Wasserstein distance (i.e., the earth moverâ€™s distance) to

èƒ½å­˜åœ¨çš„

compare these sets with one another and deï¬ne a measure of document similarity. Various researchers

distribution

have devised similarly ï¬‚avored approaches in computer vision by comparing images via calculating

the Wasserstein distance between their sets of local features. For instance, Zhou et al. [22] use this

distance to learn prototypes for image classes and perform few-shot inference, while Lin et al. [23]

leverage it for designing diverse adversarial examples. More recently, similar ideas were used for

image enhancement [24]. Finally, comparing sets via Wasserstein distances has also been proven to

be useful in other applications including graph learning [9, 12, 16], domain adaptation [25, 26], and

transfer learning [27].

In this work, we propose a novel theoretically-grounded and simple to compute permutation-invariant pooling mechanism for embedding sets of various sizes into a ï¬xed-size representation. Our proposed method, which we refer to as Pooling by Sliced-Wasserstein Embedding (PSWE), provides an exact Euclidean embedding for the (generalized) sliced-Wasserstein (SW) distance. We start by deï¬ning a similarity measure between sets of samples based on the SW distance. We then propose an explicit set embedding for which the Euclidean distance between embedded sets equals the SW distance between them. In our experiments, we follow the recent work on set learning [11, 14] and use a permutation-equivariant backbone followed by our permutation-invariant pooling method to perform end-to-end learning on different data modalities. We demonstrate the scalability and effectiveness of our approach on various learning tasks, including point cloud classiï¬cation, graph classiï¬cation, and image recognition. Aside from introducing a novel pooling mechanism, one of the key numerical insights of our work is that basic pooling mechanisms, such as mean-pooling, provide competitive performance when the permutation-equivariant backbone is complex. However, for plain backbones (e.g., a shared multi-layer perceptron (MLP) among the set elements), more sophisticated pooling mechanisms, including our proposed PSWE method as well other recently-proposed pooling mechanisms in the literature (e.g., Pooling by Multi-Head Attention (PMA) [14] and Featurewise Sort Pool (FSPool) [15]) signiï¬cantly boost the performance compared to basic pooling mechanisms.

2 Related Work
Permutation-invariant functions are crucial components in learning from sets and are often used as pooling layers to aggregate features from a set and provide a constant-size representation regardless of the set cardinality. Max, sum, and mean pooling are simple, yet very widely used, examples of such functions. Recently, various works have shown the effectiveness of more sophisticated and often learnable pooling operators in improving the performance of learning from set-structured data [13, 14, 28, 15, 10, 12]. Murphy et al. [13] introduced a pooling mechanism based on the average of a permutation equivariant-function applied to all re-orderings of the set elements. Summing over all

2

re-orderings of an input set is, of course, computationally prohibitive. Hence, one can use a canonical ordering of set elements (e.g., via sorting [15]), or learn to predict the optimal permutation for an input set [29, 30, 28].
Another prevalent idea is to perform pooling based on comparing an input set with trainable and ï¬xed-size reference sets. For instance, Skianis et al. [10] proposed a pooling that consists of the distances between an input set and trainable reference sets, where the distance was calculated by solving the correspondence problems between the input and each reference set. More interestingly, this idea is analogous to pooling by multi-head attention (PMA), an important building block in the Set Transformer and Perceiver architectures [14, 31], where the cross-attention between trainable reference sets and an input set is used as a permutation-invariant function. Attention-based pooling [14, 32] has been shown to perform really well in practice on a wide range of applications.
We introduce a novel pooling mechanism by treating sets as empirical probability measures and calculating an embedding for these probability measures in which the Euclidean distance between two embedded sets is equal to the sliced-Wasserstein distance between their empirical distributions. Our work is closely related to the work by Mialon et al. [12] and Kolouri et al. [16]. In short, [12] proposes an approximate Euclidean embedding for the Wasserstein distance, similar to [16], in a reproducing kernel Hilbert space (RKHS), while our proposed framework is based on devising an exact Euclidean embedding for the (generalized) sliced-Wasserstein distance. Interestingly, our proposed pooling by sliced-Wasserstein embedding (PSWE) can also be viewed as a theoretically-grounded generalization of the sorting-based FSPool mechanism proposed in [15], where we show that the introduction of trainable slicers, as well as trainable reference sets, further boost the end-to-end performance in a wide spectrum of classiï¬cation tasks.

3 Background

Let Xi = {xin âˆˆ Rd}Nn=i 1 denote an input set with Ni elements living in Rd. We assume that the

set elements are samples from an unknown underlying probability measure, Âµi, deï¬ned in X âŠ† Rd

with probability density dÂµi(x) = pi(x)dx, and what we have observed is the empirical distribution

pË†i(x)

=

1 Ni

Ni n=1

Î´(x

âˆ’

xin),

where

Î´(Â·)

is

the

Dirac

delta

function.

3.1 2-Wasserstein Distance

densityä¸distribution

Let Âµi and Âµj denote two Borel probability measures with ï¬nite 2nd moment deï¬ned on Xi, Xj âŠ† Rd, with corresponding probability density functions pi and pj, respectively. The 2-Wasserstein distance between Âµi and Âµj is the solution to the optimal mass transportation problem with 2 transport cost [33]:

1 2

W2(Âµi, Âµj) =

inf

xi âˆ’ xj 2dÎ³(xi, xj) ,

(1)

Î³âˆˆÎ“(Âµi,Âµj ) XiÃ—Xj

where Î“(Âµi, Âµj) is the set of all transportation plans Î³ âˆˆ Î“(Âµi, Âµj) such that Î³(A Ã— Xj) = Âµi(A) and Î³(Xi Ã— B) = Âµj(B) for any Borel subsets A âŠ† Xi and B âŠ† Xj. Due to Brenierâ€™s theorem [34], for absolutely continuous probability measures Âµi and Âµj (with respect to the Lebesgue measure), the 2-Wasserstein distance can be equivalently obtained from the Monge formulation [33],

1

2

W2(Âµi, Âµj) =

inf

x âˆ’ f (x) 2dÂµi(x) ,

(2)

f âˆˆM P (Âµi,Âµj ) X

where M P (Âµi, Âµj) = {f : Xi â†’ Xj | f#Âµi = Âµj} and f#Âµi represents the pushforward of measure Âµi, characterized as f#Âµi(B) = Âµi(f âˆ’1(B)) for any Borel subset B âŠ† Xj. The mapping f is referred to as a transport map [35], and the optimal transport map is called the Monge map. For discrete probability measures, when the transport plan Î³ is a deterministic optimal coupling, such a transport plan is referred to as a Monge coupling [33]. In case of a non-deterministic transport plan Î³,
one can obtain an approximation of the Monge coupling via barycenteric projection, e.g., see [16, 12]. In this paper, we mainly use the 2-Wasserstein distance and hereafter, for brevity, we refer to it as the
Wasserstein distance.

3

For one-dimensional probability measures, the Wasserstein distance has a closed-form solution and can be calculated as

1

1

2

W2(Âµi, Âµj) =

|FÂµâˆ’i1(Ï„ ) âˆ’ FÂµâˆ’j1(Ï„ )|2dÏ„ ,

(3)

0

where FÂµâˆ’i1 is the quantile function of Âµi. The simplicity of calculating Wasserstein distances between one-dimensional probability measures has led to the idea of sliced-Wasserstein [36, 37, 38, 39] and generalized sliced-Wasserstein [40] distances, which we will review next.

3.2 (Generalized) Sliced-Wasserstein Distances
Let gÎ¸ : Rd â†’ R be a parametric function with parameters Î¸ âˆˆ â„¦Î¸ âŠ† RdÎ¸ , satisfying the regularity conditions in both inputs and parameters as presented in [40]. For sliced-Wasserstein distance, gÎ¸(x) = Î¸T x where Î¸ âˆˆ Sdâˆ’1 is a unit vector in Rd, and Sdâˆ’1 denotes the unit d-dimensional hypersphere. The generalized slice of probability measure Âµi with respect to gÎ¸ is the one-dimensional probability measure gÎ¸#Âµi, which has the following density for all t âˆˆ R,

pÎ¸i (t) := Î´(t âˆ’ gÎ¸(x))dÂµi(x).

(4)

X

The generalized sliced-Wasserstein distance is then deï¬ned as

1

2

GSW2(Âµi, Âµj) =

W22(gÎ¸#Âµi, gÎ¸#Âµj )dÎ¸ .

(5)

â„¦Î¸

Note that for gÎ¸(x) = Î¸T x and â„¦Î¸ = Sdâˆ’1, the generalized sliced-Wasserstein distance is equivalent to the sliced-Wasserstein distance. Equation (5) is the expected value of the Wasserstein distances between slices of distributions Âµi and Âµj.

Extensions of the (generalized) sliced-Wasserstein distance include max (generalized) slicedWasserstein distance [38, 40], in which the expected value in (5) is substituted with a maximum over â„¦Î¸, i.e.,

max-GSW2(Âµi, Âµj) = max W2(gÎ¸#Âµi, gÎ¸#Âµj),

(6)

Î¸âˆˆâ„¦Î¸

subspace-robust Wasserstein distance [41], which generalizes the notion of slicing to a projection onto subspaces, and the distributional sliced-Wasserstein distance [42] that proposes to replace the expectation with respect to the uniform distribution on â„¦Î¸ with a non-uniform and learnable distribution.

From an algorithmic point of view, the expectation in (5) is approximated using Monte-Carlo integration, which results in an average of a set of Wasserstein distances between random slices of d-dimensional measures. In practice, however, GSW distances only output a good Monte-Carlo approximation using a large number of slices, while max-GSW distances achieve similar results with
only a singleGsSlicWe, çš„althè§£oué‡Šghï¼šat é‡‡theâ½¤coè’™st oç‰¹f aå¡n oæ´›ptimâ½…izæ³•atiï¼Œon è®¡oveç®—r Î¸å¤š. æ¬¡æŠ•å½±

4 PSWE: Pooling by Sliced-Wasserstein Embedding

4.1 (Generalized) Sliced-Wasserstein Embedding

We propose a Euclidean embedding for probability measures, such that the weighted Euclidean
distance between two embedded measures is equivalent to the GSW distance between them. Consider a set of probability measures {Âµi}M i=1 with densities {pi}M i=1. For simplicity of notation, let ÂµÎ¸i := gÎ¸#Âµi denote the slice of measure Âµi with respect to gÎ¸. Also, let Âµ0 denote a reference measure, with ÂµÎ¸0 representing its corresponding slice. Then, it is straightforward to show that the optimal transport map (i.e., Monge map) between ÂµÎ¸i and ÂµÎ¸0 can be written as

fiÎ¸ = FÂµâˆ’Î¸i1 â—¦ FÂµÎ¸0 ,

(7)

Fi thetaå°±æ˜¯â¼€ä¸ªmappingï¼Œå°†å½“

4

å‰çš„åˆ†å¸ƒmappingåˆ°â½¬æ ‡åˆ†å¸ƒä¸Š

where as mentioned before, FÂµâˆ’Î¸i1 and FÂµâˆ’Î¸01 respectively denote the quantile functions of ÂµÎ¸i and ÂµÎ¸0. Now, letting id denote the identity function, we can write the so-called cumulative distribution transform (CDT) [43] of ÂµÎ¸i as

Î½iÎ¸ := fiÎ¸ âˆ’ id.

(8)

For a ï¬xed Î¸, Î½iÎ¸ satisï¬es the following conditions, the proof of which can be found in the Supplementary Material:

C1: The weighted 2-norm of Î½iÎ¸ equals the Wasserstein distance between ÂµÎ¸i and ÂµÎ¸0, i.e.,

Î½iÎ¸ ÂµÎ¸0,2 =

1

2

Î½iÎ¸ (t)

2 2

dÂµÎ¸0

(t)

= W2(ÂµÎ¸i , ÂµÎ¸0),

R

hence implying that Î½0Î¸ ÂµÎ¸0,2 = 0.

C2: the weighted 2 distance between Î½iÎ¸ and Î½jÎ¸ equals the Wasserstein distance between ÂµÎ¸i

and ÂµÎ¸j , i.e.,

Î½iÎ¸ âˆ’ Î½jÎ¸ ÂµÎ¸0,2 = W2(ÂµÎ¸i , ÂµÎ¸j ).

Finally, the GSW distance between two measures, Âµi and Âµj, can be obtained as

1

1

2

2

GSW2(Âµi, Âµj) =

â„¦Î¸

Î½iÎ¸ âˆ’ Î½jÎ¸

2 ÂµÎ¸0

,2

dÎ¸

=

Î½iÎ¸(t) âˆ’ Î½jÎ¸(t)

2 2

dÂµÎ¸0

(t)dÎ¸

â„¦Î¸ R

. (9)

Based on (9), for probability measure Âµi, the mapping to the embedding space is obtained via Ï†(Âµi) := {Î½iÎ¸ | Î¸ âˆˆ â„¦Î¸}.

4.2 Algorithmic Considerations

In this section, we introduce our novel pooling algorithm, termed pooling by sliced-Wasserstein

embedding (PSWE). Let Xi = {xin âˆ¼ pi}Nn=i 1 denote an input set with Ni elements, and X0 = {x0n âˆ¼ p0}Nn=1 denote the set of N samples from a trainable reference set. Let Î˜L = {Î¸l âˆ¼ Uâ„¦Î¸ }Ll=1 denote a set of L parameters sampled uniformly from â„¦Î¸. Then, the empirical distribution of the lth

slice

of

pi

can

be

written

as

pË†Î¸i l

=

1 Ni

Ni n=1

Î´(t

âˆ’

gÎ¸l (xin)).

To

obtain

Î½iÎ¸l ,

we

need

to

calculate

the

Monge coupling between pË†Î¸i l and pË†Î¸0l . In what follows, we consider two scenarios:

1. When the input set and the reference set have the same cardinalities, i.e., Ni = N , the Monge coupling (i.e., the discrete counterpart of the Monge map shown in (7)) is obtained by sorting XiÎ¸l := {gÎ¸l (xin)}Nn=i 1 and X0Î¸l . Let Ï€i[Â·] denote the permutation indices (i.e., argsort) obtained by sorting XiÎ¸l . Then, letting Ï€0âˆ’1 denote the ordering that permutes the sorted set back to the original ordering based on sorting of elements in X0Î¸l , the Monge coupling is obtained via Ï€i[Ï€0âˆ’1[Â·]] and the per-slice embedding is calculated as

[Î½iÎ¸l ]n = gÎ¸l

xi
Ï€i [Ï€0âˆ’1 [n]]

âˆ’ gÎ¸l (x0n).

(10)

2. When the set cardinalities vary, the Monge coupling can be obtained via interpolation using

(7). In our experiments, we use the PyTorch implementation of linear interpolation2 to

evaluate

F âˆ’1
ÂµÎ¸i l

.

The

per-slice

embedding

is

calculated

as

[Î½iÎ¸l ]n

=

F âˆ’1
ÂµÎ¸i l

Ï€0âˆ’1[n] + 1 N

âˆ’ gÎ¸l (x0n),

(11)

where

FÂµÎ¸0l (x0n)

=

Ï€0âˆ’1 [n]+1 N

,

assuming

that

the

indices

start

from

0.

è¿›â¾äº†æ’å€¼

Note that, regardless of the cardinality of the input set, the per-slice embedding is N -dimensional, i.e.,

Î½iÎ¸l âˆˆ RN . The ï¬nal embedding is then deï¬ned as Ï†(Âµi) = [Î½iÎ¸1 , ..., Î½iÎ¸L ] âˆˆ RNÃ—L, which satisï¬es

GSW2(Âµi, Âµj) â‰ˆ Ï†(Âµi) âˆ’ Ï†(Âµj) F ,

(12)

where Â· F denotes the Frobenius norm, and the approximation is due to the Monte-Carlo integral approximation with the L slices.

2https://github.com/aliutkus/torchinterp1d

5

Input: Set ğ‘‹! =

ğ‘¥"!

âˆ¼

ğ‘!

%! "#$

ğ‘¥"!

ğ‘!

Ref. Set, ğ‘‹( =

ğ‘¥"(

âˆ¼

ğ‘(

% "#$

ğ‘¥"(

ğ‘(

â€¦ â€¦

Slicers (PermutationEquivariant)
ğ‘”#! : â„$ â†’ â„
ğ‘”#" : â„$ â†’ â„

Outputs of slicers, ğ‘”&" ğ‘¥"!

%! "#$

Calculate Monge Maps/Couplings, ğ‘“!&", that push ğ‘”&"#ğœ‡( to ğ‘”&"#ğœ‡!

Trainable

ğ‘”## : â„$ â†’ â„ Trainable

ğ›¼ğ‘“!&" +

1 âˆ’ ğ›¼ ğ‘–ğ‘‘

ğœ‡(
#

for ğ›¼ âˆˆ [0,1]

ğœ™(ğ‘‹() * = 0

Pooling by Sliced-Wasserstein Embedding (PSWE) is characterized by

ğœ™(ğ‘‹!) * â‰ˆ ğ’¢ğ’®ğ’² (ğœ‡!, ğœ‡()

ğœ™ ğ‘‹! âˆ’ ğœ™(ğ‘‹+) * â‰ˆ ğ’¢ğ’®ğ’² (ğœ‡!, ğœ‡+)

Permutation-Invariant Output
ğœ™ ğ‘‹! ") = ğ‘“!&" ğ‘”&" ğ‘¥"( âˆ’ ğ‘”) ğ‘¥"(

Figure 1: An overview of the proposed PSWE method. Each d-dimensional element in a given input set Xi, as well as each element in the trainable reference set X0 is passed through multiple trainable slicers {gÎ¸l }Ll=1. For each slicer, we then perform interpolation on the slicer outputs and derive the optimal transport maps that push the slicer output distributions of the reference set to the slicer output
distributions of a given set via (7), (10), and (11). The resultant transport maps are then concatenated
across all slices to derive the ï¬nal set embedding.

4.3 On Projection Complexity of Sliced Distances
Given the high-dimensional nature of the problems of interest in machine learning, and the fact that samples often live on a low-dimensional manifold, one requires a large number of random projections, L, to obtain a good approximation of the GSW distance. This issue is related to the projection complexity of the sliced distances [38, 40]. Given the dependence of our pooling dimensionality on the number of slices, L, we would like to avoid using very large numbers of slices. Here, we devise a unique approach that ties our proposed embedding to metric learning. First, we note that ideas like max-GSW [40, 38] or subspace-robust Wasserstein distance [41] would not be practical in this setting, as the slicing parameters, Î˜L, are ï¬xed for all probability measures and not chosen separately for each pairs of probability measures (Âµi, Âµj).
Given the training input sets, i.e., {Xi}M i=1, and a reference set, X0, we seek an optimal set of L slices Î˜âˆ—L that could be learned from the data alongside the other parameters in an end-to-end manner. This idea is related to [42] as it is similar to learning a distribution over the unit hypersphere from which we are sampling our L slices. The optimization on Î˜âˆ—L ties the PSWE framework to the ï¬eld of metric learning, allowing us to ï¬nd slices or, in other words, an embedding with a speciï¬c statistical characterization.
To put it all together, our pooling requires identifying: 1) the type of slicer gÎ¸ : Rd â†’ R (e.g., gÎ¸(x) = Î¸T x), 2) the number of slices, L, and 3) the number of elements in the reference set, N . Then, for an input set Xi with Ni elements, PSWE ï¬rst slices the elements of the input and reference sets with respect to slicers gÎ¸l for l âˆˆ {1, ..., L}. Then, it sorts the sliced values {gÎ¸l (xin)}Nn=i 1 and {gÎ¸l (x0n)}Nn=1 and calculates or approximates the corresponding Monge couplings according to (10) or (11), respectively. Finally, PSWE calculates the per-slice embedding Î½iÎ¸l and returns Ï†(Xi) = [Î½i1, ..., Î½iL] âˆˆ RNÃ—L. This procedure is depicted in Figure 1, as well as Algorithm 1. Note that in our proposed framework, the slicer parameters and the reference set elements are all trainable parameters that are updated using backpropagation of gradients due to the objective function of interest.
5 Experimental Evaluation
We evaluate the proposed PSWE method on a variety of point cloud, graph, and image datasets as depicted in Figure 2. For comparison, we consider four different pooling methods: Global average pooling (GAP), global max pooling (MAXâ€“evaluated on image classiï¬cation only), Pooling by
6

Algorithm 1 Pooling by Sliced Wasserstein Embedding
procedure PSWE(Xi = {xin âˆˆ Rd}Nn=i 1) Trainable parameters: Slicer parameters Î˜L âˆˆ RdÎ¸Ã—L, Reference elements X0 âˆˆ RNÃ—d for l = 1 to L do Calculate gÎ¸l (Xi) := {gÎ¸l (xin)}Nn=i 1 and gÎ¸l (X0) = {gÎ¸l (x0n)}Nn=1 Calculate Ï€i = argsort(gÎ¸l (Xi)), Ï€0 = argsort(gÎ¸l (X0)), and Ï€0âˆ’1 if Ni = N then Calculate Î½iÎ¸l according to (10) else Calculate Î½iÎ¸l according to (11) return Ï†(Xi) = [Î½iÎ¸1 , ..., Î½iÎ¸L ] âˆˆ RNÃ—L

NWPU-RESISC45

Places-Extra69

(a)

(b)

(c)

Figure 2: We evaluate the performance of PSWE and other baseline pooling methods on (a) 3D point cloud classiï¬cation from ModelNet40 dataset [44], (b) TUD graph classiï¬cation datasets [45], and (c) image recognition on NWPU-RESISC45 [46] and Places-Extra69 [47] datasets.

Multi-head Attention (PMA) [14], and Featurewise Sort Pooling (FSPool) [15]. In all the PSWE experiments, to ease the optimization process of reference elements, we optimize the reference elements at the output of the slicers rather than in the input space of the slicers. Moreover, for both PMA and PSWE, to reduce the embedding size, we use a similar weighting approach to that of FSPool, where the output embedding (which is of size N Ã— L and N Ã— d for PSWE and PMA, respectively) is multiplied elementwise by a learnable weight matrix W of the same size, and the result is summed over the rows to derive a ï¬nal L-dim and d-dim embedding for PSWE and PMA, respectively. Further details on the experiments can be found in the Supplementary Material.
5.1 Point Cloud Processing
We consider the ModelNet40 dataset [44], consisting of 3-dimensional point clouds derived from triangular meshes of 12,311 CAD models belonging to 40 object categories. We sample 1024 points uniformly at random from each object as in [48, 49] and use the ofï¬cial split, with 9,843 training samples and 2,468 test samples. We consider two different backbones, namely multi-layer perceptron (MLP) and induced set attention block (ISAB) from the Set Transformer architecture [14].
Table 1 shows the test accuracy achieved by the proposed PSWE method using different numbers of slices (L âˆˆ {1, 4, 16, 64, 256, 1024}) and the baseline pooling methods of GAP, PMA, and FSPool. As the table shows, for both backbone types, PSWE is able to outperform other pooling methods for high-enough numbers of slices. Furthermore, it is noteworthy that while mean-pooling does not perform well when using an MLP backbone, performing message passing among the set elements using the ISAB backbone signiï¬cantly boosts GAPâ€™s performance, suggesting that simple averaging of the per-element embeddings sufï¬ces to achieve a high performance level. This implies that there is an inherent trade-off between the backbone and pooling complexity, and to maintain a high accuracy level, at least one of the two components should be complex enough. A comparison of the wall-clock training and testing times of PSWE and the baseline pooling methods on the ModelNet40 dataset, as well as experimental results on visualizing the closest and farthest samples to/from the trained reference for PSWE, can be found in the Supplementary Material.
7

Backbone
MLP ISAB

GAP
57.8 Â± 0.5 86.6 Â± 0.5

PMA
86.6 Â± 0.6 87.6 Â± 0.6

FSPool
85.8 Â± 0.5 87.3 Â± 0.5

L=1 14.9 Â± 1.0 32.4 Â± 3.6

L=4 52.9 Â± 2.1 83.9 Â± 0.6

PSWE

L = 16

L = 64

77.4 Â± 0.4 83.9 Â± 0.6

86.2 Â± 0.5 86.9 Â± 0.3

L = 256 86.5 Â± 0.5 87.3 Â± 0.4

L = 1024 86.9 Â± 0.3 87.6 Â± 0.4

Table 1: Test classiï¬cation accuracy (%) of the proposed PSWE method and the baseline pooling mechanisms on the ModelNet40 point cloud dataset using multi-layer perceptron (MLP) and induced set attention block (ISAB) [14] backbones.

ENZYMES PROTEINS RDTâˆ’B IMDBâˆ’M IMDBâˆ’B

Backbone
GCN GAT GIN GCN GAT GIN GCN GAT GIN GCN GAT GIN GCN GAT GIN

GAP
69.6 Â± 3.9 73.4 Â± 3.5 73.0 Â± 5.8 51.8 Â± 4.2 49.7 Â± 3.4 49.7 Â± 2.9 81.9 Â± 2.6 75.8 Â± 3.3 81.2 Â± 3.1 69.1 Â± 5.2 69.7 Â± 4.4 69.8 Â± 6.5 25.0 Â± 5.1 24.2 Â± 5.3 29.6 Â± 6.3

PMA
74.1 Â± 5.3 70.5 Â± 7.6 70.0 Â± 8.0 50.1 Â± 2.8 49.6 Â± 4.9 50.2 Â± 3.0 82.2 Â± 2.5 76.0 Â± 3.5 77.6 Â± 7.9 72.4 Â± 5.9 72.4 Â± 6.1 72.3 Â± 4.7 32.1 Â± 4.5 28.8 Â± 3.9 30.1 Â± 4.8

FSPool
75.5 Â± 3.7 72.4 Â± 6.9 73.4 Â± 6.1 51.1 Â± 5.4 50.2 Â± 3.9 50.8 Â± 5.3 84.0 Â± 2.8 84.7 Â± 3.3 84.4 Â± 2.9 74.9 Â± 5.4 73.0 Â± 5.1 72.6 Â± 4.5 33.5 Â± 4.2 34.2 Â± 6.7 43.6 Â± 6.1

L=1 72.6 Â± 7.6 71.3 Â± 7.2 73.8 Â± 6.5 44.2 Â± 4.9 44.3 Â± 4.2 44.6 Â± 4.4 77.5 Â± 4.5 78.7 Â± 2.5 83.2 Â± 3.3 72.5 Â± 3.9 72.9 Â± 4.6 71.3 Â± 4.9 20.0 Â± 3.9 22.3 Â± 4.0 19.1 Â± 5.5

L=4 74.6 Â± 5.8 74.4 Â± 5.8 72.5 Â± 6.0 50.7 Â± 4.6 49.2 Â± 4.1 49.1 Â± 2.7 80.4 Â± 3.3 82.0 Â± 3.2 83.1 Â± 2.8 73.3 Â± 5.3 73.1 Â± 4.5 72.4 Â± 6.0 24.9 Â± 6.5 26.3 Â± 5.6 25.9 Â± 4.6

PSWE L = 16 L = 64 77.3 Â± 5.1 73.0 Â± 7.5 74.0 Â± 5.7 70.9 Â± 7.6 72.0 Â± 4.3 72.3 Â± 8.1 50.8 Â± 3.8 49.6 Â± 3.6 50.2 Â± 4.5 48.2 Â± 4.6 48.0 Â± 6.4 50.6 Â± 2.6 81.5 Â± 2.1 82.1 Â± 3.2 82.1 Â± 3.1 81.7 Â± 3.2 83.8 Â± 2.8 84.6 Â± 2.3 73.3 Â± 5.6 73.2 Â± 6.1 72.8 Â± 4.4 73.9 Â± 4.6 73.4 Â± 4.8 73.5 Â± 4.4 31.8 Â± 5.1 32.5 Â± 3.0 30.6 Â± 4.7 34.6 Â± 3.2 36.5 Â± 3.1 37.2 Â± 5.7

L = 256 72.7 Â± 5.1 73.0 Â± 3.6 74.6 Â± 7.0 51.4 Â± 4.2 47.9 Â± 4.3 50.2 Â± 3.4 81.9 Â± 3.2 83.0 Â± 3.3 83.9 Â± 2.8 72.8 Â± 6.0 74.4 Â± 4.4 73.0 Â± 5.0 37.8 Â± 4.9 38.1 Â± 5.5 45.4 Â± 7.0

L = 1024 73.5 Â± 6.1 73.4 Â± 6.0 68.8 Â± 5.1 50.2 Â± 4.7 49.4 Â± 4.5 49.5 Â± 4.0 81.7 Â± 3.0 81.7 Â± 3.7 83.7 Â± 1.7 73.9 Â± 4.6 73.7 Â± 5.5 74.9 Â± 3.9 33.7 Â± 3.9 34.9 Â± 3.9 40.0 Â± 6.0

Table 2: Cross-validation accuracy (%) of PSWE with different numbers of slices, as well as baseline pooling methods on different TUD graph classiï¬cation tasks [45] using three backbones of GCN [17], GAT [18], and GIN [19]. The best performing pooling method in each row (i.e., (dataset, backbone) pair) is highlighted in bold.

5.2 Graph Classiï¬cation
Next, we consider the prominent TUD benchmark [45] and evaluate the performance of the proposed method on ï¬ve graph classiï¬cation datasets, consisting of social network (IMDB-B, IMDB-M, REDDIT-B) and bio-informatics (ENZYMES, PROTEINS) datasets. For the former group of datasets, we use one-hot encoded degrees as initial node features, while for the latter group, we use the provided node labels as initial node features. We then pass the features, alongside the adjacency matrices, to three popular graph neural network (GNN) backbones, namely Graph Convolutional Network (GCN) [17], Graph Attention Network (GAT) [18], and Graph Isomorphism Network (GIN) [19]. Upon deriving the ï¬nal node embeddings of a given graph from a GNN backbone, we treat them as elements of a set and apply PSWE and the baseline pooling methods to derive a ï¬xed-size graph-level representation that is fed to a linear classiï¬er.
Table 2 shows the resulting 10-fold cross-validation accuracies on different datasets, and using different backbone/pooling pairings, following the evaluation methodology used in the literature [19, 50, 51]. As the table demonstrates, PSWE is able to perform similarly to or better than other pooling methods on all datasets. Furthermore, the results show that the commonly used mean-pooling for GNNs might not be the best choice, and more complex backbones might be needed to enhance the classiï¬cation performance. It is important to note that in some scenarios, especially with smaller datasets (e.g., IMDB-B / ENZYMES) and more complex backbones (e.g., GIN), the performance of PSWE does not monotonically improve with the number of slices, L, which is due to the model becoming more complex and overï¬tting the training data.
8

Dataset NWPU-RESISC45
Places-Extra69

Backbone 16 Ã— 16 Patches + MLP
ResNet18 16 Ã— 16 Patches + MLP
ResNet18

MAX 71.0 Â± 0.3 89.3 Â± 0.5 18.5 Â± 2.2 57.3 Â± 0.1

GAP 65.0 Â± 0.7 91.4 Â± 0.4 29.1 Â± 0.1 58.6 Â± 0.0

FSPool 77.2 Â± 0.6 90.7 Â± 0.3 35.4 Â± 1.8 57.3 Â± 1.8

PSWE 75.0 Â± 0.9 90.6 Â± 0.5 35.2 Â± 0.2 58.3 Â± 0.0

Table 3: Image classiï¬cation results (% test accuracy) on the NWPU-RESISC45 and Places-Extra69 datasets using two backbone types coupled with MAX, GAP, FSPool, and PSWE pooling methods.

5.3 Image Recognition
Finally, we evaluate PSWE in the context of image recognition on two large-scale image datasets: NWPU-RESISC45 [46], which is a remote sensing image scene classiï¬cation dataset comprising a total of 31,500 images belonging to 45 different aerial scene classes, and Places-Extra69 [47], which contains 98,721 training and 6,600 test images, belonging to 69 different scene categories. For processing the images, we consider two different backbone types:
â€¢ 16 Ã— 16 Patches + MLP: Inspired by the architecture used in the Vision Transformer (ViT) framework [52], we break the image into 256 patches, each ï¬‚attened into a 16Ã—16Ã—3 = 768dimensional vector, pass each patch through a shared multi-layer perceptron (MLP), add positional encoding to the MLP outputs, and treat the outputs as a set of 256 elements, each with 256 features.
â€¢ ResNet18 [53]: As an alternative, we pass the image through ResNet18, which is a convolutional neural network backbone, mapping the input image into a 7 Ã— 7 Ã— 512-dimensional tensor. We treat this tensor as a set of 49 elements, each containing 512 features.
Table 3 shows the test classiï¬cation accuracy of PSWE, as compared to GAP, MAX, and FSPool on the two datasets using the two aforementioned backbones. For PSWE, we set the number of slices to L = 1024 for the 16 Ã— 16 Patches + MLP backbone, and L = 1000 for the ResNet18 backbone. We did not include PMA results here as it performed signiï¬cantly worse than other pooling types. As the table shows, PSWE performs on par with FSPool using both backbones, and signiï¬cantly better than GAP with the simpler MLP-based backbone. This is consistent with our observation that more sophisticated pooling mechanisms can compensate the performance drop caused by simpler backbone architectures.
6 Conclusion
We introduced a novel method for permutation-invariant feature aggregation from set-structured data, called pooling by sliced-Wasserstein embedding (PSWE). Our method treats the elements of each input set as samples from a distribution, and derives a constant-size representation for the entire set based on the (generalized) sliced-Wasserstein distance between the set elements and a reference set, whose elements are learned in an end-to-end fashion, alongside with the slicer parameters. We showed that our method derives an exact Euclidean embedding which is geometrically-interpretable for setstructured data. Moreover, we demonstrated, through experimental results, that our set embedding approach outperforms baseline pooling mechanisms on a variety of supervised classiï¬cation tasks on point cloud, graph, and image datasets. While our focus in this work was on deriving global representations for input samples (such as point clouds, graphs, and images), our method is not necessarily limited to global pooling. Indeed, our approach is a generic mechanism for embedding an input set into a ï¬xed-dimensional representation and, therefore, it may also be used it for local pooling, which is an interesting direction for future work.
9

Acknowledgments and Disclosure of Funding
This material is based upon work supported by the United States Air Force under Contract No. FA8750-19-C-0098. Any opinions, ï¬ndings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reï¬‚ect the views of the United States Air Force and DARPA.
References
[1] Tony Jebara, Risi Kondor, and Andrew Howard. Probability product kernels. The Journal of Machine Learning Research, 5:819â€“844, 2004.
[2] Arthur Gretton, Karsten Borgwardt, Malte Rasch, Bernhard SchÃ¶lkopf, and Alex Smola. A kernel method for the two-sample-problem. Advances in neural information processing systems, 19:513â€“520, 2006.
[3] Oren Boiman, Eli Shechtman, and Michal Irani. In defense of nearest-neighbor based image classiï¬cation. In 2008 IEEE Conference on Computer Vision and Pattern Recognition, pages 1â€“8. IEEE, 2008.
[4] BarnabÃ¡s PÃ³czos, Liang Xiong, and Jeff Schneider. Nonparametric divergence estimation with applications to machine learning on distributions. In Proceedings of the Twenty-Seventh Conference on Uncertainty in Artiï¬cial Intelligence, pages 599â€“608, 2011.
[5] BarnabÃ¡s PÃ³czos and Jeff Schneider. Nonparametric estimation of conditional information and divergences. In Artiï¬cial Intelligence and Statistics, pages 914â€“923. PMLR, 2012.
[6] Krikamol Muandet, Kenji Fukumizu, Francesco Dinuzzo, and Bernhard SchÃ¶lkopf. Learning from distributions via support measure machines. In Proceedings of the 25th International Conference on Neural Information Processing Systems-Volume 1, pages 10â€“18, 2012.
[7] Liang Xiong and Jeff Schneider. Learning from point sets with observational bias. In Proceedings of the Thirtieth Conference on Uncertainty in Artiï¬cial Intelligence, pages 898â€“906, 2014.
[8] Soheil Kolouri, Yang Zou, and Gustavo K Rohde. Sliced-Wasserstein kernels for probability distributions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4876â€“4884, 2016.
[9] Matteo Togninalli, M Elisabetta Ghisu, Felipe Llinares-LÃ³pez, Bastian Rieck, and Karsten M Borgwardt. Wasserstein weisfeiler-lehman graph kernels. In NeurIPS, 2019.
[10] Konstantinos Skianis, Giannis Nikolentzos, Stratis Limnios, and Michalis Vazirgiannis. Rep the set: Neural networks for learning set representations. In International conference on artiï¬cial intelligence and statistics, pages 1410â€“1420. PMLR, 2020.
[11] Manzil Zaheer, Satwik Kottur, Siamak Ravanbhakhsh, BarnabÃ¡s PÃ³czos, Ruslan Salakhutdinov, and Alexander J Smola. Deep sets. In Proceedings of the 31st International Conference on Neural Information Processing Systems, pages 3394â€“3404, 2017.
[12] GrÃ©goire Mialon, Dexiong Chen, Alexandre dâ€™Aspremont, and Julien Mairal. A trainable optimal transport embedding for feature aggregation and its relationship to attention. In International Conference on Learning Representations, 2021.
[13] Ryan L. Murphy, Balasubramaniam Srinivasan, Vinayak Rao, and Bruno Ribeiro. Janossy pooling: Learning deep permutation-invariant functions for variable-size inputs. In International Conference on Learning Representations, 2019.
[14] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye Teh. Set transformer: A framework for attention-based permutation-invariant neural networks. In International Conference on Machine Learning, pages 3744â€“3753. PMLR, 2019.
[15] Yan Zhang, Jonathon Hare, and Adam PrÃ¼gel-Bennett. Fspool: Learning set representations with featurewise sort pooling. In International Conference on Learning Representations, 2020.
[16] Soheil Kolouri, Navid Naderializadeh, Gustavo K. Rohde, and Heiko Hoffmann. Wasserstein embedding for graph learning. In International Conference on Learning Representations, 2021.
[17] Thomas N. Kipf and Max Welling. Semi-supervised classiï¬cation with graph convolutional networks. In International Conference on Learning Representations (ICLR), 2017.
10

[18] Petar VelicË‡kovicÂ´, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro LiÃ², and Yoshua Bengio. Graph Attention Networks. International Conference on Learning Representations, 2018.
[19] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In International Conference on Learning Representations, 2019.
[20] Matt Kusner, Yu Sun, Nicholas Kolkin, and Kilian Weinberger. From word embeddings to document distances. In International conference on machine learning, pages 957â€“966. PMLR, 2015.
[21] Gao Huang, Chuan Guo, Matt J Kusner, Yu Sun, Fei Sha, and Kilian Q Weinberger. Supervised word mover's distance. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 29, pages 4862â€“4870. Curran Associates, Inc., 2016.
[22] Chi Zhang, Yujun Cai, Guosheng Lin, and Chunhua Shen. Deepemd: Few-shot image classiï¬cation with differentiable earth moverâ€™s distance and structured classiï¬ers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12203â€“12213, 2020.
[23] Jincheng Li, Jiezhang Cao, Shuhai Zhang, Yanwu Xu, Jian Chen, and Mingkui Tan. Internal wasserstein distance for adversarial attack and defense. arXiv preprint arXiv:2103.07598, 2021.
[24] Mauricio Delbracio, Hossein Talebi, and Peyman Milanfar. Projected distribution loss for image enhancement. arXiv preprint arXiv:2012.09289, 2020.
[25] Nicolas Courty, RÃ©mi Flamary, Devis Tuia, and Alain Rakotomamonjy. Optimal transport for domain adaptation. IEEE transactions on pattern analysis and machine intelligence, 39(9):1853â€“1865, 2017.
[26] Bharath Bhushan Damodaran, Benjamin Kellenberger, RÃ©mi Flamary, Devis Tuia, and Nicolas Courty. Deepjdot: Deep joint distribution optimal transport for unsupervised domain adaptation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 447â€“463, 2018.
[27] David Alvarez Melis and Nicolo Fusi. Geometric dataset distances via optimal transport. Advances in Neural Information Processing Systems, 33, 2020.
[28] Yan Zhang, Jonathon Hare, and Adam PrÃ¼gel-Bennett. Learning representations of sets through optimized permutations. In International Conference on Learning Representations, 2019.
[29] Oriol Vinyals, Samy Bengio, and Manjunath Kudlur. Order matters: Sequence to sequence for sets. arXiv preprint arXiv:1511.06391, 2015.
[30] S Hamid Rezatoï¬ghi, Roman Kaskman, Farbod T Motlagh, Qinfeng Shi, Daniel Cremers, Laura LealTaixÃ©, and Ian Reid. Deep perm-set net: Learn to predict sets with unknown permutation and cardinality using deep neural networks. arXiv preprint arXiv:1805.00613, 2018.
[31] Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, and Joao Carreira. Perceiver: General perception with iterative attention. arXiv preprint arXiv:2103.03206, 2021.
[32] Trieu H Trinh, Minh-Thang Luong, and Quoc V Le. Selï¬e: Self-supervised pretraining for image embedding. arXiv preprint arXiv:1906.02940, 2019.
[33] CÃ©dric Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media, 2008.
[34] Yann Brenier. Polar factorization and monotone rearrangement of vector-valued functions. Communications on pure and applied mathematics, 44(4):375â€“417, 1991.
[35] Soheil Kolouri, Se Rim Park, Matthew Thorpe, Dejan Slepcev, and Gustavo K Rohde. Optimal mass transport: Signal processing and machine-learning applications. IEEE Signal Processing Magazine, 34(4):43â€“59, 2017.
[36] Julien Rabin, Gabriel PeyrÃ©, Julie Delon, and Marc Bernot. Wasserstein barycenter and its application to texture mixing. In International Conference on Scale Space and Variational Methods in Computer Vision, pages 435â€“446. Springer, 2011.
[37] Nicolas Bonnotte. Unidimensional and evolution methods for optimal transportation. PhD thesis, UniversitÃ© Paris 11, France, 2013.
[38] Ishan Deshpande, Yuan-Ting Hu, Ruoyu Sun, Ayis Pyrros, Nasir Siddiqui, Sanmi Koyejo, Zhizhen Zhao, David Forsyth, and Alexander Schwing. Max-sliced wasserstein distance and its use for gans. In IEEE Conference on Computer Vision and Pattern Recognition, 2019.
11

[39] Julien Rabin, Gabriel PeyrÃ©, Julie Delon, and Marc Bernot. Wasserstein barycenter and its application to texture mixing. In Scale Space and Variational Methods in Computer Vision, pages 435â€“446. Springer, 2012.
[40] Soheil Kolouri, Kimia Nadjahi, Umut Simsekli, Roland Badeau, and Gustavo Rohde. Generalized sliced wasserstein distances. In Advances in Neural Information Processing Systems, pages 261â€“272, 2019.
[41] FranÃ§ois-Pierre Paty and Marco Cuturi. Subspace robust wasserstein distances. In International Conference on Machine Learning, 2019.
[42] Khai Nguyen, Nhat Ho, Tung Pham, and Hung Bui. Distributional sliced-wasserstein and applications to generative modeling. In International Conference on Learning Representations, 2021.
[43] Se Rim Park, Soheil Kolouri, Shinjini Kundu, and Gustavo K Rohde. The cumulative distribution transform and linear pattern classiï¬cation. Applied and Computational Harmonic Analysis, 45(3):616â€“641, 2018.
[44] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3d shapenets: A deep representation for volumetric shapes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1912â€“1920, 2015.
[45] Christopher Morris, Nils M. Kriege, Franka Bause, Kristian Kersting, Petra Mutzel, and Marion Neumann. Tudataset: A collection of benchmark datasets for learning with graphs. In ICML 2020 Workshop on Graph Representation Learning and Beyond (GRL+ 2020), 2020.
[46] Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sensing image scene classiï¬cation: Benchmark and state of the art. Proceedings of the IEEE, 105(10):1865â€“1883, 2017.
[47] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million image database for scene recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017.
[48] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d classiï¬cation and segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 652â€“660, 2017.
[49] Meng-Hao Guo, Jun-Xiong Cai, Zheng-Ning Liu, Tai-Jiang Mu, Ralph R Martin, and Shi-Min Hu. Pct: Point cloud transformer. arXiv preprint arXiv:2012.09688, 2020.
[50] Pinar Yanardag and SVN Vishwanathan. Deep graph kernels. In Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining, pages 1365â€“1374, 2015.
[51] Mathias Niepert, Mohamed Ahmed, and Konstantin Kutzkov. Learning convolutional neural networks for graphs. In International conference on machine learning, pages 2014â€“2023. PMLR, 2016.
[52] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021.
[53] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770â€“778, 2016.
12

