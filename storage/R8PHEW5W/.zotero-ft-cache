首发于 AI带路党
写文章
点击打开ICEY的主页
令人拍案叫绝的Wasserstein GAN
令人拍案叫绝的Wasserstein GAN
郑华滨
郑华滨
​
AI炼丹师
​ 关注他
编辑推荐
7,581 人赞同了该文章

    本文后续： Wasserstein GAN最新进展：从weight clipping到gradient penalty，更加先进的Lipschitz限制手法

在GAN的相关研究如火如荼甚至可以说是泛滥的今天，一篇新鲜出炉的arXiv论文《 Wasserstein GAN 》却在Reddit的Machine Learning频道火了，连Goodfellow都 在帖子里和大家热烈讨论 ，这篇论文究竟有什么了不得的地方呢？

要知道自从 2014年Ian Goodfellow提出 以来，GAN就存在着训练困难、生成器和判别器的loss无法指示训练进程、生成样本缺乏多样性等问题。从那时起，很多论文都在尝试解决，但是效果不尽人意，比如最有名的一个改进 DCGAN 依靠的是对判别器和生成器的架构进行实验枚举，最终找到一组比较好的网络架构设置，但是实际上是治标不治本，没有彻底解决问题。而今天的主角Wasserstein GAN（下面简称WGAN）成功地做到了以下爆炸性的几点：

    彻底解决GAN训练不稳定的问题，不再需要小心平衡生成器和判别器的训练程度
    基本解决了collapse mode的问题，确保了生成样本的多样性
    训练过程中终于有一个像交叉熵、准确率这样的数值来指示训练的进程，这个数值越小代表GAN训练得越好，代表生成器产生的图像质量越高（如题图所示）
    以上一切好处不需要精心设计的网络架构，最简单的多层全连接网络就可以做到

那以上好处来自哪里？这就是令人拍案叫绝的部分了——实际上作者整整花了两篇论文，在第一篇《 Towards Principled Methods for Training Generative Adversarial Networks 》里面推了一堆公式定理，从理论上分析了原始GAN的问题所在，从而针对性地给出了改进要点；在这第二篇《 Wasserstein GAN 》里面，又再从这个改进点出发推了一堆公式定理，最终给出了改进的算法实现流程， 而改进后相比原始GAN的算法实现流程却只改了四点 ：

    判别器最后一层去掉sigmoid
    生成器和判别器的loss不取log
    每次更新判别器的参数之后把它们的绝对值截断到不超过一个固定常数c
    不要用基于动量的优化算法（包括momentum和Adam），推荐RMSProp，SGD也行

算法截图如下：

改动是如此简单，效果却惊人地好，以至于Reddit上不少人在感叹：就这样？没有别的了？ 太简单了吧！这些反应让我想起了一个颇有年头的鸡汤段子，说是一个工程师在电机外壳上用粉笔划了一条线排除了故障，要价一万美元——画一条线，1美元；知道在哪画线，9999美元。上面这四点改进就是作者Martin Arjovsky划的简简单单四条线，对于工程实现便已足够，但是知道在哪划线，背后却是精巧的数学分析，而这也是本文想要整理的内容。

本文内容分为五个部分：

    原始GAN究竟出了什么问题？（此部分较长）
    WGAN之前的一个过渡解决方案
    Wasserstein距离的优越性质
    从Wasserstein距离到WGAN
    总结


理解原文的很多公式定理需要对测度论、 拓扑学等数学知识有所掌握，本文会从直观的角度对每一个重要公式进行解读，有时通过一些低维的例子帮助读者理解数学背后的思想，所以不免会失于严谨，如有引喻不当之处，欢迎在评论中指出。

以下简称《 Wassertein GAN 》为“WGAN本作”，简称《 Towards Principled Methods for Training Generative Adversarial Networks 》为“WGAN前作”。

WGAN源码实现： martinarjovsky/WassersteinGAN
第一部分：原始GAN究竟出了什么问题？

回顾一下，原始GAN中判别器要最小化如下损失函数，尽可能把真实样本分为正例，生成样本分为负例：

− E x ∼ P r [ log ⁡ D ( x ) ] − E x ∼ P g [ log ⁡ ( 1 − D ( x ) ) ] -\mathbb{E}_{x\sim P_r}[\log D(x)] - \mathbb{E}_{x\sim P_g}[\log(1-D(x))] （公式1 ）

其中 P r P_r 是真实样本分布， P g P_g 是由生成器产生的样本分布。对于生成器，Goodfellow一开始提出来一个损失函数，后来又提出了一个改进的损失函数，分别是

E x ∼ P g [ log ⁡ ( 1 − D ( x ) ) ] \mathbb{E}_{x\sim P_g}[\log(1-D(x))] （公式2）

E x ∼ P g [ − log ⁡ D ( x ) ] \mathbb{E}_{x\sim P_g}[- \log D(x)] （公式3）

后者在WGAN两篇论文中称为“the - log D alternative”或“the - log D trick”。WGAN前作分别分析了这两种形式的原始GAN各自的问题所在，下面分别说明。
第一种原始GAN形式的问题

一句话概括：判别器越好，生成器梯度消失越严重。 WGAN前作从两个角度进行了论证，第一个角度是从生成器的等价损失函数切入的。

首先从公式1可以得到，在生成器G固定参数时最优的判别器D应该是什么。对于一个具体的样本 x x ，它可能来自真实分布也可能来自生成分布，它对公式1损失函数的贡献是
− P r ( x ) log ⁡ D ( x ) − P g ( x ) log ⁡ [ 1 − D ( x ) ] - P_r(x) \log D(x) - P_g(x) \log [1 - D(x)]

令其关于 D ( x ) D(x) 的导数为0，得
− P r ( x ) D ( x ) + P g ( x ) 1 − D ( x ) = 0 - \frac{P_r(x)}{D(x)} + \frac{P_g(x)}{1 - D(x)} = 0

化简得最优判别器为：

D ∗ ( x ) = P r ( x ) P r ( x ) + P g ( x ) D^*(x) = \frac{P_r(x)}{P_r(x) + P_g(x)} （公式4）

这个结果从直观上很容易理解，就是看一个样本 x x 来自真实分布和生成分布的可能性的相对比例。如果 P r ( x ) = 0 P_r(x) = 0 且 P g ( x ) ≠ 0 P_g(x) \neq 0 ，最优判别器就应该非常自信地给出概率0；如果 P r ( x ) = P g ( x ) P_r(x) = P_g(x) ，说明该样本是真是假的可能性刚好一半一半，此时最优判别器也应该给出概率0.5。

然而GAN训练有一个trick，就是别把判别器训练得太好，否则在实验中生成器会完全学不动（loss降不下去），为了探究背后的原因，我们就可以看看在极端情况——判别器最优时，生成器的损失函数变成什么。给公式2加上一个不依赖于生成器的项，使之变成
E x ∼ P r [ log ⁡ D ( x ) ] + E x ∼ P g [ log ⁡ ( 1 − D ( x ) ) ] \mathbb{E}_{x\sim P_r}[\log D(x)] + \mathbb{E}_{x\sim P_g}[\log(1-D(x))]

注意，最小化这个损失函数等价于最小化公式2，而且它刚好是判别器损失函数的反。代入最优判别器即公式4，再进行简单的变换可以得到

E x ∼ P r log ⁡ P r ( x ) 1 2 [ P r ( x ) + P g ( x ) ] + E x ∼ P g log ⁡ P g ( x ) 1 2 [ P r ( x ) + P g ( x ) ] − 2 log ⁡ 2 \mathbb{E}_{x \sim P_r} \log \frac{P_r(x)}{\frac{1}{2}[P_r(x) + P_g(x)]} + \mathbb{E}_{x \sim P_g} \log \frac{P_g(x)}{\frac{1}{2}[P_r(x) + P_g(x)]} - 2\log 2 （公式5)

变换成这个样子是为了引入Kullback–Leibler divergence（简称KL散度）和Jensen-Shannon divergence（简称JS散度）这两个重要的相似度衡量指标，后面的主角之一Wasserstein距离，就是要来吊打它们两个的。所以接下来介绍这两个重要的配角——KL散度和JS散度：

K L ( P 1 | | P 2 ) = E x ∼ P 1 log ⁡ P 1 P 2 KL(P_1||P_2) = \mathbb{E}_{x \sim P_1} \log \frac{P_1}{P_2} （公式6）

J S ( P 1 | | P 2 ) = 1 2 K L ( P 1 | | P 1 + P 2 2 ) + 1 2 K L ( P 2 | | P 1 + P 2 2 ) JS(P_1 || P_2) = \frac{1}{2}KL(P_1||\frac{P_1 + P_2}{2}) + \frac{1}{2}KL(P_2||\frac{P_1 + P_2}{2}) （公式7）

于是公式5就可以继续写成
2 J S ( P r | | P g ) − 2 log ⁡ 2 2JS(P_r || P_g) - 2\log 2

（公式8）

到这里读者可以先喘一口气，看看目前得到了什么结论： 根据原始GAN定义的判别器loss，我们可以得到最优判别器的形式；而在最优判别器的下，我们可以把原始GAN定义的生成器loss等价变换为最小化真实分布 P r P_r 与生成分布 P g P_g 之间的JS散度。我们越训练判别器，它就越接近最优，最小化生成器的loss也就会越近似于最小化 P r P_r 和 P g P_g 之间的JS散度。

问题就出在这个JS散度上。我们会希望如果两个分布之间越接近它们的JS散度越小，我们通过优化JS散度就能将 P g P_g “拉向” P r P_r ，最终以假乱真。这个希望在两个分布有所重叠的时候是成立的，但是如果两个分布完全没有重叠的部分，或者它们重叠的部分可忽略（下面解释什么叫可忽略），它们的JS散度是多少呢？

答案是 log ⁡ 2 \log 2 ，因为对于任意一个x只有四种可能：

P 1 ( x ) = 0 P_1(x) = 0 且 P 2 ( x ) = 0 P_2(x) = 0

P 1 ( x ) ≠ 0 P_1(x) \neq 0 且 P 2 ( x ) ≠ 0 P_2(x) \neq 0

P 1 ( x ) = 0 P_1(x) = 0 且 P 2 ( x ) ≠ 0 P_2(x) \neq 0

P 1 ( x ) ≠ 0 P_1(x) \neq 0 且 P 2 ( x ) = 0 P_2(x) = 0

第一种对计算JS散度无贡献，第二种情况由于重叠部分可忽略所以贡献也为0，第三种情况对公式7右边第一个项的贡献是 log ⁡ P 2 1 2 ( P 2 + 0 ) = log ⁡ 2 \log \frac{P_2}{\frac{1}{2}(P_2 + 0)} = \log 2 ，第四种情况与之类似，所以最终 J S ( P 1 | | P 2 ) = log ⁡ 2 JS(P_1||P_2) = \log 2 。

换句话说，无论 P r P_r 跟 P g P_g 是远在天边，还是近在眼前，只要它们俩没有一点重叠或者重叠部分可忽略，JS散度就固定是常数 log ⁡ 2 \log 2 ， 而这对于梯度下降方法意味着——梯度为0 ！此时对于最优判别器来说，生成器肯定是得不到一丁点梯度信息的；即使对于接近最优的判别器来说，生成器也有很大机会面临梯度消失的问题。

但是 P r P_r 与 P g P_g 不重叠或重叠部分可忽略的可能性有多大？不严谨的答案是：非常大。比较严谨的答案是： 当 P r P_r 与 P g P_g 的支撑集（support）是高维空间中的低维流形（manifold）时， P r P_r 与 P g P_g 重叠部分测度（measure）为0的概率为1。

不用被奇怪的术语吓得关掉页面，虽然论文给出的是严格的数学表述，但是直观上其实很容易理解。首先简单介绍一下这几个概念：

    支撑集（support）其实就是函数的非零部分子集，比如ReLU函数的支撑集就是 ( 0 , + ∞ ) (0, +\infty) ，一个概率分布的支撑集就是所有概率密度非零部分的集合。
    流形（manifold）是高维空间中曲线、曲面概念的拓广，我们可以在低维上直观理解这个概念，比如我们说三维空间中的一个曲面是一个二维流形，因为它的本质维度（intrinsic dimension）只有2，一个点在这个二维流形上移动只有两个方向的自由度。同理，三维空间或者二维空间中的一条曲线都是一个一维流形。
    测度（measure）是高维空间中长度、面积、体积概念的拓广，可以理解为“超体积”。

回过头来看第一句话，“当 P r P_r 与 P g P_g 的支撑集是高维空间中的低维流形时”，基本上是成立的。原因是GAN中的生成器一般是从某个低维（比如100维）的随机分布中采样出一个编码向量，再经过一个神经网络生成出一个高维样本（比如64x64的图片就有4096维）。当生成器的参数固定时，生成样本的概率分布虽然是定义在4096维的空间上，但它本身所有可能产生的变化已经被那个100维的随机分布限定了，其本质维度就是100，再考虑到神经网络带来的映射降维，最终可能比100还小，所以生成样本分布的支撑集就在4096维空间中构成一个最多100维的低维流形，“撑不满”整个高维空间。

“撑不满”就会导致真实分布与生成分布难以“碰到面”，这很容易在二维空间中理解：一方面，二维平面中随机取两条曲线，它们之间刚好存在重叠线段的概率为0；另一方面，虽然它们很大可能会存在交叉点，但是相比于两条曲线而言，交叉点比曲线低一个维度，长度（测度）为0，可忽略。三维空间中也是类似的，随机取两个曲面，它们之间最多就是比较有可能存在交叉线，但是交叉线比曲面低一个维度，面积（测度）是0，可忽略。从低维空间拓展到高维空间，就有了如下逻辑：因为一开始生成器随机初始化，所以 P g P_g 几乎不可能与 P r P_r 有什么关联，所以它们的支撑集之间的重叠部分要么不存在，要么就比 P r P_r 和 P g P_g 的最小维度还要低至少一个维度，故而测度为0。所谓“重叠部分测度为0”，就是上文所言“不重叠或者重叠部分可忽略”的意思。

我们就得到了WGAN前作中关于生成器梯度消失的第一个论证： 在（近似）最优判别器下，最小化生成器的loss等价于最小化 P r P_r 与 P g P_g 之间的JS散度，而由于 P r P_r 与 P g P_g 几乎不可能有不可忽略的重叠，所以无论它们相距多远JS散度都是常数 log ⁡ 2 \log 2 ，最终导致生成器的梯度（近似）为0，梯度消失。

接着作者写了很多公式定理从第二个角度进行论证，但是背后的思想也可以直观地解释：

    首先， P r P_r 与 P g P_g 之间几乎不可能有不可忽略的重叠，所以无论它们之间的“缝隙”多狭小，都肯定存在一个最优分割曲面把它们隔开，最多就是在那些可忽略的重叠处隔不开而已。
    由于判别器作为一个神经网络可以无限拟合这个分隔曲面，所以存在一个最优判别器，对几乎所有真实样本给出概率1，对几乎所有生成样本给出概率0，而那些隔不开的部分就是难以被最优判别器分类的样本，但是它们的测度为0，可忽略。
    最优判别器在真实分布和生成分布的支撑集上给出的概率都是常数（1和0），导致生成器的loss梯度为0，梯度消失。

有了这些理论分析，原始GAN不稳定的原因就彻底清楚了：判别器训练得太好，生成器梯度消失，生成器loss降不下去；判别器训练得不好，生成器梯度不准，四处乱跑。只有判别器训练得不好不坏才行，但是这个火候又很难把握，甚至在同一轮训练的前后不同阶段这个火候都可能不一样，所以GAN才那么难训练。

实验辅证如下：

    WGAN前作Figure 2。先分别将DCGAN训练1，20，25个epoch，然后固定生成器不动，判别器重新随机初始化从头开始训练，对于第一种形式的生成器loss产生的梯度可以打印出其尺度的变化曲线，可以看到随着判别器的训练，生成器的梯度均迅速衰减。注意y轴是对数坐标轴。

第二种原始GAN形式的问题

一句话概括：最小化第二种生成器loss函数，会等价于最小化一个不合理的距离衡量，导致两个问题，一是梯度不稳定，二是collapse mode即多样性不足。 WGAN前作又是从两个角度进行了论证，下面只说第一个角度，因为对于第二个角度我难以找到一个直观的解释方式，感兴趣的读者还是去看论文吧（逃）。

如前文所说，Ian Goodfellow提出的“- log D trick”是把生成器loss改成

E x ∼ P g [ − log ⁡ D ( x ) ] \mathbb{E}_{x\sim P_g}[- \log D(x)] （公式3）

上文推导已经得到在最优判别器 D ∗ D^* 下

E x ∼ P r [ log ⁡ D ∗ ( x ) ] + E x ∼ P g [ log ⁡ ( 1 − D ∗ ( x ) ) ] = 2 J S ( P r | | P g ) − 2 log ⁡ 2 \mathbb{E}_{x\sim P_r}[\log D^*(x)] + \mathbb{E}_{x\sim P_g}[\log(1-D^*(x))] = 2JS(P_r || P_g) - 2\log 2 （公式9）

我们可以把KL散度（注意下面是先g后r）变换成含 D ∗ D^* 的形式：

K L ( P g | | P r ) = E x ∼ P g [ log ⁡ P g ( x ) P r ( x ) ] = E x ∼ P g [ log ⁡ P g ( x ) / ( P r ( x ) + P g ( x ) ) P r ( x ) / ( P r ( x ) + P g ( x ) ) ] = E x ∼ P g [ log ⁡ 1 − D ∗ ( x ) D ∗ ( x ) ] = E x ∼ P g log ⁡ [ 1 − D ∗ ( x ) ] − E x ∼ P g log ⁡ D ∗ ( x ) \begin{align} KL(P_g || P_r) &= \mathbb{E}_{x \sim P_g} [\log \frac{P_g(x)}{P_r(x)}] \\ &= \mathbb{E}_{x \sim P_g} [\log \frac{P_g(x) / (P_r(x) + P_g(x))}{P_r(x) / (P_r(x) + P_g(x))}] \\ &= \mathbb{E}_{x \sim P_g} [\log \frac{1 - D^*(x)}{D^*(x)}] \\ &= \mathbb{E}_{x \sim P_g} \log [1 - D^*(x)] - \mathbb{E}_{x \sim P_g} \log D^*(x) \end{align} \\ （公式10）

由公式3，9，10可得最小化目标的等价变形
E x ∼ P g [ − log ⁡ D ∗ ( x ) ] = K L ( P g | | P r ) − E x ∼ P g log ⁡ [ 1 − D ∗ ( x ) ] = K L ( P g | | P r ) − 2 J S ( P r | | P g ) + 2 log ⁡ 2 + E x ∼ P r [ log ⁡ D ∗ ( x ) ] \begin{align} \mathbb{E}_{x \sim P_g} [-\log D^*(x)] &= KL(P_g || P_r) - \mathbb{E}_{x \sim P_g} \log [1 - D^*(x)] \\ &= KL(P_g || P_r) - 2JS(P_r || P_g) + 2\log 2 + \mathbb{E}_{x\sim P_r}[\log D^*(x)] \end{align}

注意上式最后两项不依赖于生成器G，最终得到最小化公式3等价于最小化

K L ( P g | | P r ) − 2 J S ( P r | | P g ) KL(P_g || P_r) - 2JS(P_r || P_g) （公式11）

这个等价最小化目标存在两个严重的问题。第一是它同时要最小化生成分布与真实分布的KL散度，却又要最大化两者的JS散度，一个要拉近，一个却要推远！这在直观上非常荒谬，在数值上则会导致梯度不稳定，这是后面那个JS散度项的毛病。

第二，即便是前面那个正常的KL散度项也有毛病。因为KL散度不是一个对称的衡量， K L ( P g | | P r ) KL(P_g || P_r) 与 K L ( P r | | P g ) KL(P_r || P_g) 是有差别的。以前者为例

    当 P g ( x ) → 0 P_g(x)\rightarrow 0 而 P r ( x ) → 1 P_r(x)\rightarrow 1 时， P g ( x ) log ⁡ P g ( x ) P r ( x ) → 0 P_g(x) \log \frac{P_g(x)}{P_r(x)} \rightarrow 0 ，对 K L ( P g | | P r ) KL(P_g || P_r) 贡献趋近0
    当 P g ( x ) → 1 P_g(x)\rightarrow 1 而 P r ( x ) → 0 P_r(x)\rightarrow 0 时， P g ( x ) log ⁡ P g ( x ) P r ( x ) → + ∞ P_g(x) \log \frac{P_g(x)}{P_r(x)} \rightarrow +\infty ，对 K L ( P g | | P r ) KL(P_g || P_r) 贡献趋近正无穷

换言之， K L ( P g | | P r ) KL(P_g || P_r) 对于上面两种错误的惩罚是不一样的，第一种错误对应的是“生成器没能生成真实的样本”，惩罚微小；第二种错误对应的是“生成器生成了不真实的样本” ，惩罚巨大。第一种错误对应的是缺乏多样性，第二种错误对应的是缺乏准确性。 这一放一打之下，生成器宁可多生成一些重复但是很“安全”的样本，也不愿意去生成多样性的样本，因为那样一不小心就会产生第二种错误，得不偿失。这种现象就是大家常说的collapse mode。

第一部分小结：在原始GAN的（近似）最优判别器下，第一种生成器loss面临梯度消失问题，第二种生成器loss面临优化目标荒谬、梯度不稳定、对多样性与准确性惩罚不平衡导致mode collapse这几个问题。

实验辅证如下：

    WGAN前作Figure 3。先分别将DCGAN训练1，20，25个epoch，然后固定生成器不动，判别器重新随机初始化从头开始训练，对于第二种形式的生成器loss产生的梯度可以打印出其尺度的变化曲线，可以看到随着判别器的训练，蓝色和绿色曲线中生成器的梯度迅速增长，说明梯度不稳定，红线对应的是DCGAN相对收敛的状态，梯度才比较稳定。

第二部分：WGAN之前的一个过渡解决方案

原始GAN问题的根源可以归结为两点，一是等价优化的距离衡量（KL散度、JS散度）不合理，二是生成器随机初始化后的生成分布很难与真实分布有不可忽略的重叠。

WGAN前作其实已经针对第二点提出了一个解决方案，就是对生成样本和真实样本加噪声，直观上说，使得原本的两个低维流形“弥散”到整个高维空间，强行让它们产生不可忽略的重叠。而一旦存在重叠，JS散度就能真正发挥作用，此时如果两个分布越靠近，它们“弥散”出来的部分重叠得越多，JS散度也会越小而不会一直是一个常数，于是（在第一种原始GAN形式下）梯度消失的问题就解决了。在训练过程中，我们可以对所加的噪声进行退火（annealing），慢慢减小其方差，到后面两个低维流形“本体”都已经有重叠时，就算把噪声完全拿掉，JS散度也能照样发挥作用，继续产生有意义的梯度把两个低维流形拉近，直到它们接近完全重合。以上是对原文的直观解释。

在这个解决方案下我们可以放心地把判别器训练到接近最优，不必担心梯度消失的问题。而当判别器最优时，对公式9取反可得判别器的最小loss为
min L D ( P r + ϵ , P g + ϵ ) = − E x ∼ P r + ϵ [ log ⁡ D ∗ ( x ) ] − E x ∼ P g + ϵ [ log ⁡ ( 1 − D ∗ ( x ) ) ] = 2 log ⁡ 2 − 2 J S ( P r + ϵ | | P g + ϵ ) \begin{align} \min L_D(P_{r+\epsilon}, P_{g+\epsilon}) &= - \mathbb{E}_{x\sim P_{r+\epsilon}}[\log D^*(x)] - \mathbb{E}_{x\sim P_{g+\epsilon}}[\log(1-D^*(x))] \\ &= 2\log 2 - 2JS(P_{r+\epsilon} || P_{g+\epsilon}) \end{align}

其中 P r + ϵ P_{r+\epsilon} 和 P g + ϵ P_{g+\epsilon} 分别是加噪后的真实分布与生成分布。反过来说，从最优判别器的loss可以反推出当前两个加噪分布的JS散度。两个加噪分布的JS散度可以在某种程度上代表两个原本分布的距离，也就是说可以通过最优判别器的loss反映训练进程！……真的有这样的好事吗？

并没有，因为加噪JS散度的具体数值受到噪声的方差影响，随着噪声的退火，前后的数值就没法比较了，所以它不能成为 P r P_r 和 P g P_g 距离的本质性衡量。

因为本文的重点是WGAN本身，所以WGAN前作的加噪方案简单介绍到这里，感兴趣的读者可以阅读原文了解更多细节。 加噪方案是针对原始GAN问题的第二点根源提出的，解决了训练不稳定的问题，不需要小心平衡判别器训练的火候，可以放心地把判别器训练到接近最优，但是仍然没能够提供一个衡量训练进程的数值指标。但是WGAN本作就从第一点根源出发，用Wasserstein距离代替JS散度，同时完成了稳定训练和进程指标的问题！

作者未对此方案进行实验验证。
第三部分：Wasserstein距离的优越性质

Wasserstein距离又叫Earth-Mover（EM）距离，定义如下：

W ( P r , P g ) = inf γ ∼ Π ( P r , P g ) E ( x , y ) ∼ γ [ | | x − y | | ] W(P_r, P_g) = \inf_{\gamma \sim \Pi (P_r, P_g)} \mathbb{E}_{(x, y) \sim \gamma} [||x - y||] （公式12）

解释如下： Π ( P r , P g ) \Pi (P_r, P_g) 是 P r P_r 和 P g P_g 组合起来的所有可能的联合分布的集合，反过来说， Π ( P r , P g ) \Pi (P_r, P_g) 中每一个分布的边缘分布都是 P r P_r 和 P g P_g 。对于每一个可能的联合分布 γ \gamma 而言，可以从中采样 ( x , y ) ∼ γ (x, y) \sim \gamma 得到一个真实样本 x x 和一个生成样本 y y ，并算出这对样本的距离 | | x − y | | ||x-y|| ，所以可以计算该联合分布 γ \gamma 下样本对距离的期望值 E ( x , y ) ∼ γ [ | | x − y | | ] \mathbb{E}_{(x, y) \sim \gamma} [||x - y||] 。在所有可能的联合分布中能够对这个期望值取到的下界 inf γ ∼ Π ( P r , P g ) E ( x , y ) ∼ γ [ | | x − y | | ] \inf_{\gamma \sim \Pi (P_r, P_g)} \mathbb{E}_{(x, y) \sim \gamma} [||x - y||] ，就定义为Wasserstein距离。

直观上可以把 E ( x , y ) ∼ γ [ | | x − y | | ] \mathbb{E}_{(x, y) \sim \gamma} [||x - y||] 理解为在 γ \gamma 这个“路径规划”下把 P r P_r 这堆“沙土”挪到 P g P_g “位置”所需的“消耗”，而 W ( P r , P g ) W(P_r, P_g) 就是“最优路径规划”下的“最小消耗”，所以才叫Earth-Mover（推土机）距离。

Wasserstein距离相比KL散度、JS散度的优越性在于，即便两个分布没有重叠，Wasserstein距离仍然能够反映它们的远近。 WGAN本作通过简单的例子展示了这一点。考虑如下二维空间中的两个分布 P 1 P_1 和 P 2 P_2 ， P 1 P_1 在线段AB上均匀分布， P 2 P_2 在线段CD上均匀分布，通过控制参数 θ \theta 可以控制着两个分布的距离远近。

此时容易得到（读者可自行验证）

K L ( P 1 | | P 2 ) = K L ( P 1 | | P 2 ) = { + ∞ if  θ ≠ 0 0 if  θ = 0 KL(P_1 || P_2) = KL(P_1 || P_2) = \begin{cases} +\infty & \text{if $\theta \neq 0$} \\ 0 & \text{if $\theta = 0$} \end{cases} （突变）

J S ( P 1 | | P 2 ) = { log ⁡ 2 if  θ ≠ 0 0 if  θ − 0 JS(P_1||P_2)= \begin{cases} \log 2 & \text{if $\theta \neq 0$} \\ 0 & \text{if $\theta - 0$} \end{cases} （突变）

W ( P 0 , P 1 ) = | θ | W(P_0, P_1) = |\theta| （平滑）

KL散度和JS散度是突变的，要么最大要么最小， Wasserstein距离却是平滑的 ，如果我们要用梯度下降法优化 θ \theta 这个参数，前两者根本提供不了梯度，Wasserstein距离却可以。类似地，在高维空间中如果两个分布不重叠或者重叠部分可忽略，则KL和JS既反映不了远近，也提供不了梯度， 但是Wasserstein却可以提供有意义的梯度 。
第四部分：从Wasserstein距离到WGAN

既然Wasserstein距离有如此优越的性质，如果我们能够把它定义为生成器的loss，不就可以产生有意义的梯度来更新生成器，使得生成分布被拉向真实分布吗？

没那么简单，因为Wasserstein距离定义（公式12）中的 inf γ ∼ Π ( P r , P g ) \inf_{\gamma \sim \Pi (P_r, P_g)} 没法直接求解，不过没关系，作者用了一个已有的定理把它变换为如下形式

W ( P r , P g ) = 1 K sup | | f | | L ≤ K E x ∼ P r [ f ( x ) ] − E x ∼ P g [ f ( x ) ] W(P_r, P_g) = \frac{1}{K} \sup_{||f||_L \leq K} \mathbb{E}_{x \sim P_r} [f(x)] - \mathbb{E}_{x \sim P_g} [f(x)] （公式13）

证明过程被作者丢到论文附录中了，我们也姑且不管，先看看上式究竟说了什么。

首先需要介绍一个概念——Lipschitz连续。它其实就是在一个连续函数 f f 上面额外施加了一个限制，要求存在一个常数 K ≥ 0 K\geq 0 使得定义域内的任意两个元素 x 1 x_1 和 x 2 x_2 都满足
| f ( x 1 ) − f ( x 2 ) | ≤ K | x 1 − x 2 | |f(x_1) - f(x_2)| \leq K |x_1 - x_2|

此时称函数 f f 的Lipschitz常数为 K K 。

简单理解，比如说 f f 的定义域是实数集合，那上面的要求就等价于 f f 的导函数绝对值不超过 K K 。再比如说 log ⁡ ( x ) \log (x) 就不是Lipschitz连续，因为它的导函数没有上界。Lipschitz连续条件限制了一个连续函数的最大局部变动幅度。

公式13的意思就是在要求函数 f f 的Lipschitz常数 | | f | | L ||f||_L 不超过 K K 的条件下，对所有可能满足条件的 f f 取到 E x ∼ P r [ f ( x ) ] − E x ∼ P g [ f ( x ) ] \mathbb{E}_{x \sim P_r} [f(x)] - \mathbb{E}_{x \sim P_g} [f(x)] 的上界，然后再除以 K K 。特别地，我们可以用一组参数 w w 来定义一系列可能的函数 f w f_w ，此时求解公式13可以近似变成求解如下形式

K ⋅ W ( P r , P g ) ≈ max w : | f w | L ≤ K E x ∼ P r [ f w ( x ) ] − E x ∼ P g [ f w ( x ) ] K \cdot W(P_r, P_g) \approx \max_{w: |f_w|_L \leq K} \mathbb{E}_{x \sim P_r} [f_w(x)] - \mathbb{E}_{x \sim P_g} [f_w(x)] （公式14）

再用上我们搞深度学习的人最熟悉的那一套，不就可以把 f f 用一个带参数 w w 的神经网络来表示嘛！由于神经网络的拟合能力足够强大，我们有理由相信，这样定义出来的一系列 f w f_w 虽然无法囊括所有可能，但是也足以高度近似公式13要求的那个 s u p | | f | | L ≤ K sup_{||f||_L \leq K} 了。

最后，还不能忘了满足公式14中 | | f w | | L ≤ K ||f_w||_L \leq K 这个限制。我们其实不关心具体的K是多少，只要它不是正无穷就行，因为它只是会使得梯度变大 K K 倍，并不会影响梯度的方向。所以作者采取了一个非常简单的做法，就是限制神经网络 f θ f_\theta 的所有参数 w i w_i 的不超过某个范围 [ − c , c ] [-c, c] ，比如 w i ∈ [ − 0.01 , 0.01 ] w_i \in [- 0.01, 0.01] ，此时关于输入样本 x x 的导数 ∂ f w ∂ x \frac{\partial f_w}{\partial x} 也不会超过某个范围，所以一定存在某个不知道的常数 K K 使得 f w f_w 的局部变动幅度不会超过它，Lipschitz连续条件得以满足。具体在算法实现中，只需要每次更新完 w w 后把它clip回这个范围就可以了。

到此为止，我们可以构造一个含参数 w w 、最后一层不是非线性激活层的判别器网络 f w f_w ，在限制 w w 不超过某个范围的条件下，使得

L = E x ∼ P r [ f w ( x ) ] − E x ∼ P g [ f w ( x ) ] L = \mathbb{E}_{x \sim P_r} [f_w(x)] - \mathbb{E}_{x \sim P_g} [f_w(x)] （公式15）

尽可能取到最大，此时 L L 就会近似真实分布与生成分布之间的Wasserstein距离（忽略常数倍数 K K ）。注意原始GAN的判别器做的是真假二分类任务，所以最后一层是sigmoid，但是现在WGAN中的判别器 f w f_w 做的是近似拟合Wasserstein距离，属于回归任务，所以要把最后一层的sigmoid拿掉。

接下来生成器要近似地最小化Wasserstein距离，可以最小化 L L ，由于Wasserstein距离的优良性质，我们不需要担心生成器梯度消失的问题。再考虑到 L L 的第一项与生成器无关，就得到了WGAN的两个loss。

− E x ∼ P g [ f w ( x ) ] - \mathbb{E}_{x \sim P_g} [f_w(x)] （公式16，WGAN生成器loss函数）

E x ∼ P g [ f w ( x ) ] − E x ∼ P r [ f w ( x ) ] \mathbb{E}_{x \sim P_g} [f_w(x)]- \mathbb{E}_{x \sim P_r} [f_w(x)] （公式17，WGAN判别器loss函数）

公式15是公式17的反，可以指示训练进程，其数值越小，表示真实分布与生成分布的Wasserstein距离越小，GAN训练得越好。

WGAN完整的算法流程已经贴过了，为了方便读者此处再贴一遍：

上文说过，WGAN与原始GAN第一种形式相比，只改了四点：

    判别器最后一层去掉sigmoid
    生成器和判别器的loss不取log
    每次更新判别器的参数之后把它们的绝对值截断到不超过一个固定常数c
    不要用基于动量的优化算法（包括momentum和Adam），推荐RMSProp，SGD也行

前三点都是从理论分析中得到的，已经介绍完毕；第四点却是作者从实验中发现的，属于trick，相对比较“玄”。作者发现如果使用Adam，判别器的loss有时候会崩掉，当它崩掉时，Adam给出的更新方向与梯度方向夹角的cos值就变成负数，更新方向与梯度方向南辕北辙，这意味着判别器的loss梯度是不稳定的，所以不适合用Adam这类基于动量的优化算法。作者改用RMSProp之后，问题就解决了，因为RMSProp适合梯度不稳定的情况。

对WGAN作者做了不少实验验证，本文只提比较重要的三点。第一，判别器所近似的Wasserstein距离与生成器的生成图片质量高度相关，如下所示（此即题图）：

第二，WGAN如果用类似DCGAN架构，生成图片的效果与DCGAN差不多：
但是厉害的地方在于WGAN不用DCGAN各种特殊的架构设计也能做到不错的效果，比如如果大家一起拿掉Batch Normalization的话，DCGAN就崩了：

如果WGAN和原始GAN都使用多层全连接网络（MLP），不用CNN，WGAN质量会变差些，但是原始GAN不仅质量变得更差，而且还出现了collapse mode，即多样性不足：

第三，在所有WGAN的实验中未观察到collapse mode，作者也只说应该是解决了，

最后补充一点论文没提到，但是我个人觉得比较微妙的问题。判别器所近似的Wasserstein距离能够用来指示单次训练中的训练进程，这个没错；接着作者又说它可以用于比较多次训练进程，指引调参，我倒是觉得需要小心些。比如说我下次训练时改了判别器的层数、节点数等超参，判别器的拟合能力就必然有所波动，再比如说我下次训练时改了生成器两次迭代之间，判别器的迭代次数，这两种常见的变动都会使得Wasserstein距离的拟合误差就与上次不一样。 那么这个拟合误差的变动究竟有多大，或者说不同的人做实验时判别器的拟合能力或迭代次数相差实在太大，那它们之间还能不能直接比较上述指标，我都是存疑的。

评论区的知友 @Minjie Xu 进一步指出，相比于判别器迭代次数的改变， 对判别器架构超参的改变会直接影响到对应的Lipschitz常数 K K ，进而改变近似Wasserstein距离的倍数，前后两轮训练的指标就肯定不能比较了， 这是需要在实际应用中注意的。对此我想到了一个工程化的解决方式，不是很优雅：取同样一对生成分布和真实分布，让前后两个不同架构的判别器各自拟合到收敛，看收敛到的指标差多少倍，可以近似认为是后面的 K 2 K_2 相对前面 K 1 K_1 的变化倍数，于是就可以用这个变化倍数校正前后两轮训练的指标。
第五部分：总结

WGAN前作分析了Ian Goodfellow提出的原始GAN两种形式各自的问题，第一种形式等价在最优判别器下等价于最小化生成分布与真实分布之间的JS散度，由于随机生成分布很难与真实分布有不可忽略的重叠以及JS散度的突变特性，使得生成器面临梯度消失的问题；第二种形式在最优判别器下等价于既要最小化生成分布与真实分布直接的KL散度，又要最大化其JS散度，相互矛盾，导致梯度不稳定，而且KL散度的不对称性使得生成器宁可丧失多样性也不愿丧失准确性，导致collapse mode现象。

WGAN前作针对分布重叠问题提出了一个过渡解决方案，通过对生成样本和真实样本加噪声使得两个分布产生重叠，理论上可以解决训练不稳定的问题，可以放心训练判别器到接近最优，但是未能提供一个指示训练进程的可靠指标，也未做实验验证。

WGAN本作引入了Wasserstein距离，由于它相对KL散度与JS散度具有优越的平滑特性，理论上可以解决梯度消失问题。接着通过数学变换将Wasserstein距离写成可求解的形式，利用一个参数数值范围受限的判别器神经网络来最大化这个形式，就可以近似Wasserstein距离。在此近似最优判别器下优化生成器使得Wasserstein距离缩小，就能有效拉近生成分布与真实分布。WGAN既解决了训练不稳定的问题，也提供了一个可靠的训练进程指标，而且该指标确实与生成样本的质量高度相关。作者对WGAN进行了实验验证。

（可自由转载，注明来源和作者即可。 ）
编辑于 2017-04-20 22:18
「真诚赞赏，手留余香」
赞赏

46 人已赞赏
赞赏用户 赞赏用户 赞赏用户 赞赏用户 赞赏用户
深度学习（Deep Learning）
生成对抗网络（GAN）
人工智能
​ 赞同 7581 ​ ​ 380 条评论
​ 分享
​ 喜欢 ​ 收藏 ​ 申请转载
​
评论千万条，友善第一条

380 条评论
默认
最新
Philip
Philip
我添加一点关于GAN的宏观背景吧。GAN是属于机器学习中generative中的implicit model的一种。
Generative体现在：GAN并不能计算数据真实分布的公式，也就是不能计算概率，但它能根据学习到的数据真实分布来生成一个样本。
implicit体现在：它的模型是通过网络层实现的，并不是一个确定的数学公式，好比高斯分布等。
VAE，GAN这些生成模型终极目标是模拟数据的真实分布，模拟的好坏自然得有个测距公式来计算：
VAE里面是用KL divegence来计算两个分布的距离。
GAN里面可以理解成是用Jessen-Shannon divegence来计算两个分布的距离。
我们常说GAN是一个min-max训练过程，所谓的max其实是对应着鉴别网络，目的是为了训练鉴别网络让其等同于最优JS divence的作用，然后在这个最优的测距网络下，min生成网络。
现在要说到WS-GAN了，它的最大贡献是（个人观点）指出了KL,JS等这些测距工具都有一个缺点，那就是不连续性，意思就是两个分布的差距是跳跃的，不是连续的，这就导致训练鉴别网络时很不稳定，然后作者提出了WS divegence这个测距工具，WS算出来的两个分布的差距是连续的， 用它来代替鉴别网络（撤换掉sigmoid等），因为是连续，所以训练的时候你可以很清晰的看到鉴别网络的loss是逐步的减小，整个训练过程稳定下来了。

只是根据自己的记忆大概的写一下，所以难免很多地方不准确，多谅解，只是希望大家能够有个更高的视角去看待GAN，有很多文章都在divegence这块做贡献，谢谢。
2017-02-07 · 热评
​ 回复 ​ 278
王喆
王喆
是啊，我有一个问题，正如文末所说，其实em距离和js距离，甚至是tv距离，都是Integral Probability Metrics 的特殊化，在该种视野下，似乎可以统一gan的一些模型，比如energy－based gan，gan，wgan等模型，或者说给出一种直觉上的比较，不知道这是不是你最后所说的高视角？谢谢
2017-02-17
​ 回复 ​ 2
火腿肠和方便面
火腿肠和方便面

请问用 KL divegence来计算两个分布的距离具体应该如何操作？例如度量两个图片数据集的分布距离，公式里的p1和p2分别应该如何代入呢？


2018-05-17
​ 回复 ​ 赞
展开其他 1 条回复 ​
曹恭泽
曹恭泽
写的很好！补充两点：
1. WGAN 的优越性质本质在于，如果 G 满足一定光滑性条件，那么 EM 距离对 \theta 总是可导的，而 KL 与 JS 则否。
2. paper 里还证明了 EBGAN 的 loss 是在 minimize TV 距离，而 TV 距离与 JS 是等价的（事实上 KL > TV = JS > EM ）。
我的 tensorflow 实现： Zardinality/WGAN-tensorflow 。
2017-02-05 · 热评
​ 回复 ​ 182
曹恭泽
曹恭泽
王喆
一点不错。第一点， EM 给的是 weak* 拓扑，粗到足以使其关于参数连续。第二点，TV 就是{连续有界函数}*上的范数，当然很强。
2017-02-17
​ 回复 ​ 1
孤云独去闲
孤云独去闲
谢谢分享
2017-02-06
​ 回复 ​ 1
查看全部 11 条回复 ​
Dr.Wu
Dr.Wu
​
写的真好，本身这篇论文就是很有insight的，没有直接用gan拍任务吹牛，这篇知乎中文讲的也简洁易懂。
2017-02-05
​ 回复 ​ 31
郑华滨
郑华滨
作者
论文大概由于篇幅限制主要写数学推导，直观解读相对少一些，所以我就写得多一些嘿嘿
2017-02-05
​ 回复 ​ 21
Dr.Wu
Dr.Wu
​
郑华滨
嗯 这个论文我在他挂arxiv时候看过，数学推导对于计算机系的学生还挺复杂～ 你写的比较易懂
2017-02-05
​ 回复 ​ 4
查看全部 6 条回复 ​
Philip
Philip
添加一点，在WS-GAN中，也是一个min-max训练过程，max正是对应了WS divegence（取一个上确界）的公式，所以max其实就是等同于将鉴别网络改造成一个最优的WS测距工具，所以min-max可以这么理解：
max：让鉴别网络逐步变成一个最优的WS-divegence；
min：在当前WS-divegence（鉴别网络）下，减少真实数据分布与模型分布的距离；
所以我觉得鉴别网络这个词就变的不是很准确了，鉴别网络扮演的角色是WS-divegence，自然没有什么sigmoid东西了。
谢谢
2017-02-07 · 热评
​ 回复 ​ 62
郑华滨
郑华滨
作者
经朋友提醒，之前没有解释清楚为什么判别器最后一层去掉sigmoid，现在补上了：

"注意原始GAN的判别器做的是真假二分类任务，所以最后一层是sigmoid，但是现在WGAN中的判别器做的是近似拟合Wasserstein距离，属于回归任务，所以要把最后一层的sigmoid拿掉。"
2017-02-05
​ 回复 ​ 43
林远
林远
​
最火的模型的最深刻的研究的最科普的文章
2017-02-05
​ 回复 ​ 38
郑华滨
郑华滨
作者
一针见血
2017-02-06
​ 回复 ​ 6
风兮雨兮
风兮雨兮
代码实现： WassersteinGAN/README.md at master · martinarjovsky/WassersteinGAN · GitHub - 故事贴
PyTorch 的
2017-02-05
​ 回复 ​ 15
tcsxc
tcsxc
你好，能否问一下这个loss会出现负值吗
2022-04-28
​ 回复 ​ 1
huyz
huyz

请问如果用WGAN或者WGAN-GP作为损失函数，判别器的loss和生成器的loss是不是会出现负值？我在做实验的过程中D和G的损失均出现了负值，是我代码写错了还是确实会出现负值，希望大佬能给解释一下（从损失函数上来看，感觉确实有可能会出现负值）
2019-03-05
​ 回复 ​ 6
JuleJuleJule
JuleJuleJule

从W-loss的公式来看这个loss应该是正的，但是你调的优化器应该做的是最小化，所以优化判别器的时候你的W-loss应该要取个反，这个时候就成负的了。对于生成器，你优化的时候因为W-loss的第一项其实是和G无关的，所以可以直接优化第二项，而第二项显然是个负值。
2022-06-10
​ 回复 ​ 1
蹦哒哒
蹦哒哒

我也是，D的loss收敛到负值。为什么D衡量两个分布之间的距离不加绝对值呢？D收敛到负值是表示G更靠近真实分布吗？
2019-10-03
​ 回复 ​ 1
查看全部 7 条回复 ​
james
james
tql 2021年来看还是很有收获
2021-10-02
​ 回复 ​ 4
NaChuaN
NaChuaN
​

2023还是很有收获哈哈
02-15
​ 回复 ​ 4
kenny
kenny

我有一个问题想请教一下大家，怎么保证公式17是正数？我实验怎么公式17算出来都是负数？从公式15的最大化讲，判别器要最大化loss，也就是加大距离。公式17是公式15的反，那么公式17需要让值越来越小，结合我实验上的负数，可以解释为公式17的判别器要loss是个绝对值越来越大的负数，其实从数学上讲是越来越小。那么楼主说的 “公式15是公式17的反，可以指示训练进程，其数值越小，表示真实分布与生成分布的Wasserstein距离越小，GAN训练得越好。” 就不对了，应该是距离越来越大。

总结一下，如果说 “数值越小，表示真实分布与生成分布的Wasserstein距离越小，GAN训练得越好。”这句话是针对原始公式15而言的，那么针对公式17，这个描述也应该反过来。

对么？ 还请大家给与解答。谢谢！
2017-06-19
​ 回复 ​ 3
哈吼
哈吼
​
我也发现了和你一样的问题，我做实验也是是负数，并且越来越小
2020-01-09
​ 回复 ​ 1
烟雨平生
烟雨平生
时隔多年才看到。个人理解大概是判别器训练过程中,公式17代表的loss肯定越来越小（绝对值大的负数）。Wasserstein距离越来越小是相对于生成器训练过程来说的，生成器糊弄判别器，会导致公式17增大（绝对值小的负数），反过来Wasserstein距离就会减小。
2021-09-09
​ 回复 ​ 赞
王坐龙
王坐龙

请问为什么JS散度算出来是log2，我怎么都理解不了。。。。
2018-08-01
​ 回复 ​ 2
辰辰无敌
辰辰无敌

log会把乘除变成加减。这个是为了凑Js散度提出来的系数，变成了减法
03-29
​ 回复 ​ 赞
Tian Zhou
Tian Zhou
但是看reddit的讨论， Ian Goodfellow 似乎并没有impressive的感觉，大量挑刺，citation的问题，over claim的问题，几乎就没有一句好话……“What's new in this paper is to make the cost purely linear for the discriminator too.” 对文章的正面评论就只有这一句。要说这会是best paper还说的早了点……
2017-03-10
​ 回复 ​ 2
郑华滨
郑华滨
作者
我觉得他简直阴阳怪气
2017-03-10
​ 回复 ​ 4
legenda
legenda

想问问楼主用wgan训练了吗 效果怎么样呢 我用自己的数据 效果很差
2021-02-04
​ 回复 ​ 2
Jiaming Liu
Jiaming Liu
best paper预定
2017-02-05
​ 回复 ​ 12
郑华滨
郑华滨
作者
要是没best paper那真是说不过去
2017-02-05
​ 回复 ​ 8
Dr.Wu
Dr.Wu
​
best paper 还需要人脉。。。
2017-02-05
​ 回复 ​ 3
Gandor26
Gandor26
看paper跟看电影一样激动是第一次
2017-02-06
​ 回复 ​ 7
勃失败
勃失败
如何分享专栏文章到收藏夹？
2017-02-05
​ 回复 ​ 7
郑华滨
郑华滨
作者
惊现勃勃！好像知乎不支持这个功能吧
2017-02-05
​ 回复 ​ 8
张思朋
张思朋
郑华滨

app可以
2017-03-08
​ 回复 ​ 赞
查看全部 6 条回复 ​
郑华滨
郑华滨
作者
补充内容：“最后补充一点论文没提到，但是我个人觉得比较微妙的问题。判别器所近似的Wasserstein距离能够用来指示单次训练中的训练进程，这个没错；接着作者又说它可以用于比较多次训练进程，指引调参，我倒是觉得需要小心些。比如说我下次训练时改了判别器的层数、节点数等超参，判别器的拟合能力就必然有所波动，再比如说我下次训练时改了生成器两次迭代之间，判别器的迭代次数，这两种常见的变动都会使得Wasserstein距离的拟合误差就与上次不一样。那么这个拟合误差的变动究竟有多大，或者说不同的人做实验时判别器的拟合能力或迭代次数相差实在太大，那它们之间还能不能直接比较上述指标，我都是存疑的。”
2017-02-06
​ 回复 ​ 6
Minjie Xu
Minjie Xu
郑华滨
刚想到一个更加彻底，甚至可能是比WGAN更加有效的办法！然后Google了一把，发现已经被人做了，投在今年的ICLR。。。
其实框架跟WGAN类似，就是把Wasserstein Distance换成Maximum Mean Discrepancy，然后MMD有个类似的等价形式，那里不需要Lipchitz假设。
相关链接供参考：
https:// arxiv.org/abs/1611.0448 8
http://www. inference.vc/another-fa vourite-machine-learning-paper-adversarial-networks-vs-kernel-scoring-rules/
2017-02-07 · 热评
​ 回复 ​ 109
Softmax
Softmax
Minjie Xu
还好你不是因为地方不够没有把这个美妙的想法写下来
2018-07-29
​ 回复 ​ 9
查看全部 10 条回复 ​
我不是那样的烟火
我不是那样的烟火
nice!!!一下午看的居然错过饭点了，跟着回忆了一些高等代数和实变里面的东西
2017-02-09
​ 回复 ​ 3
刘俊是
刘俊是
有一次感受到数学的美妙
2017-02-06
​ 回复 ​ 3
云飞扬
云飞扬
测度，拓扑，我先哭会
2017-02-05
​ 回复 ​ 3
郑华滨
郑华滨
作者
非数学专业的我表示也去哭会
2017-02-05
​ 回复 ​ 2
点击查看全部评论
评论千万条，友善第一条

文章被以下专栏收录

    AI带路党
    AI带路党
    带路光荣，解放人类
    自监督学习
    自监督学习
    撰写和收录自监督学习以及他的变种学习方面的文章。
    统计与机器学习
    统计与机器学习

推荐阅读

    令人拍案叫绝的Wasserstein GAN
    令人拍案叫绝的Wasserstein GAN
    无止境
    我的Gal开发日志二十：十月开发日志
    我的Gal开发日志二十：十月开发日志
    林叶 发表于Galga...
    Vulkan移植GPUImage总结
    Vulkan移植GPUImage总结
    天天不在
    minori解散了，那么国内的这些Gal开发者们日子过得怎么样
    minori解散了，那么国内的这些Gal开发者们日子过得怎么样
    windc... 发表于ACG批评

