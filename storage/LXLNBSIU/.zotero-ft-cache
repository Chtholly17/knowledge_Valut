SLOSH: Set LOcality Sensitive Hashing via Sliced-Wasserstein Embeddings

Yuzhe Lu‚àó Computer Science Department,
Vanderbilt University, Nashville, TN, 37235
yuzhe.lu@vanderbilt.edu
Andrea Soltoggio School of Computer Science,
Loughborough University, Leicestershire, UK
a.soltoggio@lboro.ac.uk

Xinran Liu‚àó Computer Science Department,
Vanderbilt University Nashville, TN, 37235
xinran.liu@vanderbilt.edu
Soheil Kolouri Computer Science Department,
Vanderbilt University, Nashville, TN, 37235
soheil.kolouri@vanderbilt.edu

arXiv:2112.05872v2 [cs.LG] 8 Feb 2022

Abstract
Learning from set-structured data is an essential problem with many applications in machine learning and computer vision. This paper focuses on non-parametric and data-independent learning from set-structured data using approximate nearest neighbor (ANN) solutions, particularly locality-sensitive hashing. We consider the problem of set retrieval from an input set query. Such retrieval problem requires: 1) an efÔ¨Åcient mechanism to calculate the distances/dissimilarities between sets, and 2) an appropriate data structure for fast nearest neighbor search. To that end, we propose Sliced-Wasserstein set embedding as a computationally efÔ¨Åcient ‚Äúset-2-vector‚Äù mechanism that enables downstream ANN, with theoretical guarantees. The set elements are treated as samples from an unknown underlying distribution, and the Sliced-Wasserstein distance is used to compare sets. We demonstrate the effectiveness of our algorithm, denoted as Set-LOcality Sensitive Hashing (SLOSH), on various set retrieval datasets and compare our proposed embedding with standard set embedding approaches, including Generalized Mean (GeM) embedding/pooling, Featurewise Sort Pooling (FSPool), and Covariance Pooling and show consistent improvement in retrieval results. The code for replicating our results is available here: https://github.com/mint-vu/SLOSH.
1. Introduction
The nearest neighbor search problem is at the heart of many nonparametric learning approaches in classiÔ¨Åcation,

regression, and density estimation, with many applications in machine learning, computer vision, and other related Ô¨Åelds [3, 6, 55]. The exhaustive search solution to the nearest neighbor problem for N given objects (e.g., images, vectors, etc.) requires N evaluation of (dis)similarities (or distances), which could be problematic when: 1) the number of objects, N , is large, or 2) (dis)similarity evaluation is expensive. Approximate Nearest Neighbor (ANN) [5] approaches have been proposed as an efÔ¨Åcient alternative for similarity search on massive datasets. ANN approaches leverage data structures like random projections, e.g., Locality-Sensitive Hashing (LSH) [17, 21], or tree-based structures, e.g., kdtrees [10, 63], to reduce the complexity of nearest neighbor search. Ideally, ANN approaches must address both these challenges, i.e., decreasing the number of similarity evaluations and reducing the computational complexity of similarity calculations while providing theoretical guarantees on ANN retrievals.
Despite the great strides in developing ANN methods, the majority of the existing approaches are designed for objects living in Hilbert spaces. Recently, however, there has been an increasing interest in set-structured data with many applications in point cloud processing, graph learning, image/video recognition, and object detection, to name a few [36,62,70]. Even when the input data itself is not a set, in many applications, the complex input data (e.g., a natural image or a graph) is decomposed into a set of more abstract components (e.g., objects or node embeddings). Similarity search for large databases of set-structured data remains an active Ô¨Åeld of research, with many real-world applications. In this paper, we focus on developing a data-independent

ANN method for set-structured data. We leverage insights from computational optimal transport [13, 30, 48, 61] and propose a novel LSH algorithm, which relies on SlicedWasserstein Embeddings and enables efÔ¨Åcient set retrieval.
DeÔ¨Åning (dis)similarities for set-structured data comes with unique challenges: i) the sets could have different cardinalities, and ii) the set elements do not necessarily have an inherent ordering. Hence, a similarity measure for setstructured data must handle varied input sizes and should be invariant to permutations, i.e., the (dis)similarity score should not change under any permutation of the input set elements. Generally, the existing approaches for deÔ¨Åning similarities between sets rely on the following two strategies. First, solving an assignment problem (via optimization) for Ô¨Ånding corresponding elements between two sets and aggregate (dis)similarities between corresponding elements, e.g., using Hungarian algorithm, Wasserstein distances, Chamfer loss, etc. These approaches are at best quadratic and at worst cubic in the set cardinalities.
The second family of approaches rely on embedding the sets into a vector space and leveraging common similarities in the embedded space. The set embedding could be explicit (e.g., deep set networks) [36, 70] or implicit (e.g., Kernel methods) [11, 19, 22, 32, 42, 50, 51, 69]. Also, the embedding process could be data-dependent (i.e., learning based) as in deep set learning approaches, which leverage a composition of permutation-equivariant backbones followed by a permutation-invariant global pooling mechanisms that deÔ¨Åne a parametric permutation-invariant set embedding into a Hilbert space [36,70,72]. Or, it can be data-independent as it is the case for global average/max/sum/covariance pooling, variations of Janossy pooling [43], and variations of Wasserstein embedding [27], among others. Recently, there has been a lot of interest in learning-based embeddings using deep neural networks and in particular transformer networks. However, data-independent embedding approaches (e.g., global poolings) have received less attention.
Contributions. Our paper focuses on non-parametric learning from set-structured data using data-independent set embeddings. Precisely, we consider the problem where our training data is a set of sets, i.e., X = {Xi|Xi = {xin ‚àà Rd}Nn=i‚àí01}Ii=1, (e.g., set of point clouds), and for a query set X we would like to retrieve the K-Nearest Neighbors (KNN) from X . To solve this problem, we require a fast and reliable (dis)similarity measure between sets, and a computationally efÔ¨Åcient nearest neighbor search. We propose Sliced-Wasserstein Embedding as a computationally efÔ¨Åcient and powerful tool that provides a set-2-vector operator with computational complexity of O(LN (d + log(N )), with sequential processing, O(N (d + log(N )), with parallel processing, where L is of the same order as d. Treating sets as empirical distributions, Sliced-Wasserstein Embedding embedds sets in a vector space in which the Eu-
LÔºö‰πüËÆ∏ÊòØËøõ‚æèÊäïÂΩ±ÁöÑÊ¨°Êï∞Ôºü

clidean distance between two embedded vectors is equal to the Sliced-Wasserstein distance between their corresponding empirical distributions. Such embedding enables the application of fast ANN approaches, like Locality Sensitive Hashing (LSH), to sets while providing collision probabilities with respect to the Sliced-Wasserstein distance. Finally, we provide extensive numerical results analyzing and comparing our approach with various data-independent embedding methods in the literature.
2. Related Work
Set embeddings (set-2-vector): Machine learning on set-structured data is challenging due to: 1) permutationinvariant nature of sets, and 2) having various cardinalities. Hence, any model (parametric or non-parametric) designed for analyzing set-structured data has to be permutation invariant, and allow for inputs of various sizes. Today, a common approach for learning from sets is to use a permutation equivariant parametric function, e.g., fully connected networks [70] or transformer networks [36], composed with a permutation invariant function, i.e., a global pooling, e.g., global average pooling, or pooling by multi-head attention [36]. One can view this process as embedding a set into a Ô¨Åxed-dimensional representation through a parametric embedding that could be learned using the training data and an objective function (e.g., classiÔ¨Åcation).
A major unanswered question is regarding nonparametric learning from set-structured data. In other words, what would be a good data-independent set embedding that one can use for generic applications, including KNearest Neighbor classiÔ¨Åcation/regression/density estimation? Given that a data-independent global pooling could be viewed as a set-2-vector process, we surveyed the existing set-2-vector mechanisms in the literature. In particular, global average/max/sum and covariance pooling [64] could be considered as the simplest such processes. Generalized Mean (GeM) [53] is another pooling mechanism commonly used in image retrieval applications, which captures higher statistical moments of the underlying distributions. Other notable approaches include VLAD [7, 23], CroW [25], and FSPool [72], among others.
Locality Sensitive Hashing (LSH): A LSH function hashes two ‚Äúsimilar‚Äù objects into the same bucket with ‚Äúhigh‚Äù probability, while ensuring that ‚Äúdissimilar‚Äù objects will end up in the same bucket with ‚Äúlow‚Äù probability. Originally presented in [21] and extended in [17], LSH uses random projections of high-dimensional data to hash samples into different buckets. The LSH algorithm forms the foundation of many ANN search methods, which provide theoretical guarantees and have been extensively studied since its conception [3, 4, 6, 33].
Here, we are interested in nearest neighbor retrieval for sets. More precisely, given a training set of sets as training

data (think of set of point clouds), and a test set (a point that the Euclidean distance between embedded sets is equal

cloud representation of an object) we would like to retrieve to the SW-distance between their corresponding empirical

the ‚Äúnearest‚Äù sets in our training set in an efÔ¨Åcient manner. distributions. But Ô¨Årst, let us brieÔ¨Çy deÔ¨Åne the Wasserstein

To that end, we extend LSH to enable its application to set and Sliced-Wasserstein distances.

retrieval. While there has been a few recent work [26, 45]

Let ¬µi and ¬µj be one-dimensional probability measures

on the topic of LSH for set queries, our proposed approach deÔ¨Åned on R. Then the p-Wasserstein distance between

signiÔ¨Åcantly differs from these work. In contrast to [26,45], these measures can be written as:

we provide a Euclidean embedding for sets, which allows for a direct utilization of the LSH algorithm and provides collision probabilities as a function of the set metrics.

Wp(¬µi, ¬µj) =

1
(F¬µ‚àíi1(œÑ ) ‚àí F¬µ‚àíj1(œÑ ))pdœÑ

1 p

(1)

0

Wasserstein-based learning: Wasserstein distances are rooted in the optimal transportation problem [30, 48, 61], and they provide a robust mathematical framework for comparing probability distributions that respect the underlying geometry of the space. Wasserstein distances have recently received abundant interest from the Machine Learning and Computer Vision communities. These distances and their variations (e.g., Sliced-Wasserstein distances [13, 52] and subspace robust Wasserstein distances [47]) have been extensively studied in the context of deep generative modeling [8,20,31,38,60], domain adaptation [9,15,16,35], transfer learning [2], adversarial attacks [66, 67], and adversarial robustness [37, 58].
More recently, Wasserstein distances and optimal transport have been used in the context of comparing setstructured data. The main idea behind these recent approaches is to treat sets (with possibly variable cardinalities) as empirical distributions and use transport-based distances for comparing/modeling these distributions. For instance, Togninalli et al. [59] propose to compare node embeddings of two graphs (treated as sets) via the Wasserstein distance. Later, Mialon et al. [40] and Kolouri et al. [27] concurrently propose Wasserstein embedding frameworks for extracting

where F¬µ‚àí1 is the inverse of the cumulative distribution function (c.d.f) F¬µ of ¬µ, i.e., it is the quantile function. The

one-dimensional p-Wasserstein distance for empirical dis-

tributions with N and M samples can be computed with

O(N log(N ) + M log(M )), which is in stark difference

from the generally cubic order for (d>1)-dimensional distri-

butions. The one-dimensional case motivates the concept of

Sliced-Wasserstein distances [28,52]. For the rest of this pa-

per we will consider only the case p = 2, and for brevity we

refer to 2-Wasserstein and 2-Sliced-Wasserstein distances

as Wasserstein and Sliced-Wasserstein distances.

The main idea behind SW distances is to slice d-

dimensional distributions into inÔ¨Ånite sets of their one-

dimensional slices/marginals and then calculate the ex-

pected Wasserstein distance between their slices. Let gŒ∏ : Rd ‚Üí R be a parametric function with parameters Œ∏ ‚àà ‚Ñ¶Œ∏ ‚äÜ RdŒ∏ , satisfying the regularity conditions in both in-

puts and parameters as presented in [28]. A common choice

is gŒ∏(x) = Œ∏T x where Œ∏ ‚àà Sd‚àí1 is a unit vector in Rd, and Sd‚àí1 denotes the unit d-dimensional hypersphere. The slice
of a probability measure, ¬µ, with respect to gŒ∏ is the onedimensional probability measure gŒ∏#¬µ, with the density,

Â∏∏ËßÑÁöÑÊäï ÂΩ±ÔºöÁ∫øÊÄß ÊäïÂΩ±

Ô¨Åxed-dimensional representations from set-structured data. Here, we further extend this direction and propose SlicedWasserstein Embedding as a computationally efÔ¨Åcient ap-

pŒ∏i (t) := Œ¥(t ‚àí gŒ∏(x))d¬µi(x) ‚àÄt ‚àà R.

(2)

X

proach that allows us to perform data-independent non- The generalized Sliced-Wasserstein distance is deÔ¨Åned as

parametric learning from set-structured data.
pi(x): Ë°®3.Áé∞Pr‰∫ÜelximniiÁöÑnaÂàÜrieÂ∏És ÔºåxniÁöÑÂØÜÂ∫¶ÔºàdensityÔºâÂáΩÊï∞ÔºåmuiÔºöGË°®SWÁ§∫2(d¬µii,s¬µtjr)in=utio‚Ñ¶nŒ∏ W22(gŒ∏#¬µi, gŒ∏#¬µj)dœÉ(Œ∏)

1 2
, (3)
ÊäïÂΩ±ÁªìÊûúÂú®ÂèÇÊï∞

We denote an input set with Ni elements living in Rd by Xi = {xin ‚àà Rd}Nn=i‚àí01. We view sets as empirical probability measures, ¬µi, deÔ¨Åned in X ‚äÜ Rd with probability

where œÉ(Œ∏) for gŒ∏(x) = Wasserstein

diŒ∏siTstthxaenacunendiifs‚Ñ¶osrŒ∏mim=mplSeyadts‚àíhu1er,eStohlineceg‚Ñ¶deŒ∏-nW,earanasdlsizeoernsdtceÁ©∫ Â∏∏ ‚ΩÖeSinlaicgÈó¥ Èáá Ê≥ïdeaidisn‰∏ä ‚Ω§-- ÁßØ ËíôÂàÜ ÁâπÔºå Âç°ÈÄö Ê¥õ

density

d¬µi(x)

=

pi(x)dx,

where

pi(x)

=

1 Ni

Ni n=1

Œ¥(x

‚àí

tance. Equation (3) is the expected value of the Wasserstein

xin), and Œ¥(¬∑) is the Dirac delta function. The main idea is distances between uniformly distributed slices (i.e., on ‚Ñ¶Œ∏)

then to deÔ¨Åne the distance between two sets, Xi and Xj, as of distributions ¬µi and ¬µj.

a probability metric between their empirical distributions.

3.2. Locality Sensitive Hashing

3.1. Sliced-Wasserstein distances

Locality Sensitive Hashing (LSH) [17, 21] strives for

We use the Sliced-Wasserstein (SW) probability metric hashing near points in the high-dimensional input-space

as a distance measure between the sets (viewed as empirical into the same hash bucket with a higher probability than

probability distributions). Later we will see that this choice allows us to effectively embed sets into a vector space such

distant ones, effectively solving the (R, c)-Near Neighbor problem. More precisely, let H := {h : Rd ‚Üí U } denote a

LSHÁöÑÊÄßË¥®

LSH function family with U denoting the hash values. The
LSH function family H, is called (R, c, P1, P2)-sensitive if for any two points u, v ‚àà Rd and ‚àÄh ‚àà H it satisÔ¨Åes the
following conditions:

‚Ä¢ If u ‚àí v 2 ‚â§ R, then Pr[h(u) = h(v)] ‚â• P1, and

‚Ä¢ If u ‚àí v 2 > cR, then Pr[h(u) = h(v)] ‚â§ P2

Where for a proper LSH c > 1 and P1 > P2. The original LSH for Euclidean distance uses,

aT u + b

ha,b(u) =

œâ

where ¬∑ is the Ô¨Çoor function, a ‚àà Rd is a random vector generated from a p-stable distribution (e.g., Normal or Cauchy distributions) [17], and b is a real number chosen uniformly from [0, œâ) such that œâ is the width of the hash bucket. Let r = u ‚àí v p and fp(t) denote the probability density function of the absolute value of the p-stable distribution, then this family of hash functions leads to the following probability of collision:

œâ1 t

t

Pr[ha,b(u) = ha,b(v)] =

0

r fp( r )(1

‚àí

)dt œâ

(4)

To amplify the gap between P1 and P2 one can concatenate several such hash functions. Let G := {g : Rd ‚Üí U k} denote the concatenation of k randomly selected hash func-
tions, i.e., g(u) = (h1(u), ..., hk(u)). In practice, multiple such hash functions g1, . . . , gT are often used. For points, u and v, within R-distance from one another, the probability that they collide at least in one of gjs is: 1 ‚àí (1 ‚àí P1k)T .
Another commonly used, and related, family of hash
functions consist of random projections and thresholding, i.e., ha,b(u) = sgn(aT u + b) [14]. Using k such random projections (i.e., binary codes of length k) provides the fol-
lowing collision probability:

Pr[g(u) = g(v)] = [1 ‚àí cos‚àí1(uT v) ]k œÄ
Since the conception of its idea [17, 21], many variants of LSH have been proposed. However, these approaches are not designed to handle set queries. Here, we extend LSH to set-structured data via Sliced-Wasserstein Embeddings.

4. Problem Formulation and Method
4.1. Sliced-Wasserstein Embedding
The idea of Sliced-Wasserstein Embeddings (SWE) is rooted in Linear Optimal Transport [30,41,65] and was Ô¨Årst introduced in the context of pattern recognition from 2D probability density functions (e.g., images) [29] and more recently in [56]. Our work extends the framework to ddimensional distributions with the speciÔ¨Åc application of set

retrieval in mind. Consider a set of probability measures {¬µi}Ii=1 with densities {pi}Ii=1, and recall that we use ¬µi to represent the i‚Äôth set Xi = {xin}Nn=i‚àí01. At a high level, SWE can be thought as a set-2-vector operator, œÜ, such that:

generaliœÜz(e¬µdi) ‚àí œÜ(¬µj) 2 = GSW2(¬µi, ¬µj).

(5)

For convenience, we use ¬µŒ∏i := gŒ∏#¬µi to denote the slice

of measure ¬µi with respect to gŒ∏. Also, let ¬µ0 denote a ref-

erence measure, with ¬µŒ∏0 being its corresponding slice. The

optimal transport coupling (i.e., Monge coupling) between

¬µŒ∏i and ¬µŒ∏0 can be written as

transport planÔºöminimize

TiŒ∏ = F¬µ‚àíŒ∏i1 ‚ó¶ F¬µŒ∏0 ,

(6)

and recall that F¬µ‚àíŒ∏i1 is the quantile function of ¬µŒ∏i . Now, letting id denote the identity function, we deÔ¨Åne the so-called
cumulative distribution transform (CDT) [46] of ¬µŒ∏i as

œÜŒ∏(¬µi) := (TiŒ∏ ‚àí id).

(7)

For a Ô¨Åxed Œ∏, we can show that œÜŒ∏(¬µi) satisÔ¨Åes (see supplementary material):

C1. The weighted 2-norm of the embedded slice, œÜŒ∏(¬µi), satisÔ¨Åes:

œÜŒ∏(¬µi) ¬µŒ∏0,2 =

1

2

œÜŒ∏ (¬µi (t))

2 2

d¬µŒ∏0

(t)

R

= W2(¬µŒ∏i , ¬µŒ∏0),

As a corollary we have œÜŒ∏(¬µ0) ¬µŒ∏0,2 = 0.
C2. The weighted 2 distance between two embedded slices satisÔ¨Åes:

œÜŒ∏(¬µi) ‚àí œÜŒ∏(¬µj ) ¬µŒ∏0,2 = W2(¬µŒ∏i , ¬µŒ∏j ). (8)

It follows from C1 and C2 that:

GSW2(¬µi, ¬µj ) =

1

‚Ñ¶Œ∏

œÜŒ∏(¬µi) ‚àí œÜŒ∏(¬µj )

2 ¬µŒ∏0

,2

dœÉ(Œ∏)

2

(9)

For probability measure ¬µi, we then deÔ¨Åne the mapping to the embedding space via,

œÜ(¬µi) := {œÜŒ∏(¬µi) | Œ∏ ‚àà ‚Ñ¶Œ∏}.
Finally, we can re-weight the embedding (according to d¬µŒ∏0) such that the weighted 2 in 8 becomes the 2 distance as in 5. Next, we describe this reweighting and other implementation considerations in more details.

Input Set,

ùëã' =

ùë•('

‚àà

‚Ñù#

)! ($%

Random set of ùêø vectors sampled from ùïä#*%

ùïä#*%

ùïä#*%

‚Ä¶

ùïä#*%

ùúÉ!

ùúÉ"

ùúÉ#

Reference Set,

ùëã! =

ùë•"!

‚àà ‚Ñù#

& "$%

1

2 3

ùëî!! (ùëã" ) ùëî!!(ùëã#) 2 1 3

ùëî!" (ùëã" ) ùëî!" (ùëã# )

12 3

ùúã"!!
Sort
ùúã"!! 2 1 3

ùúã"!"
Sort
ùúã"!"

12 3

ùëá'+"
21 3
ùëá'+#
12 3

‚Ä¶ ‚Ä¶

ùúô ùëã" ‚àà ‚Ñù$%
ùúô% ùêøùëÄ
ùúô+# ùêøùëÄ

Figure 1. A graphical depiction of Sliced-Wasserstein Embedding (SWE) for a given reference set and a chosen number of slices, L. The input and reference sets are sliced via L random projections {gŒ∏l }Ll=1. The projections are then sorted and the Monge couplings between input and reference sets‚Äô slices are calculated following Eq. (13). Finally, the embedding is obtained by weighting and concatenating
œÜŒ∏l s. The Euclidean distance between two embedded sets is equal to their corresponding GSW2,L distance/dissimilarity measure, i.e.,
œÜ(Xi) ‚àí œÜ(Xj ) 2 = GSW2,L(Xi, Xj ) ‚âà GSW2(Xi, Xj ).
Sliced Wasserstein Embedding

4.2. Monte Carlo Integral Approximation

4.3. SWE Algorithm

The GSW distance in Eq. (3) relies on integration on ‚Ñ¶Œ∏ (e.g., Sd‚àí1 for linear slices), which cannot be directly

calculated. Following the common practice in the litera-

ture [12, 31, 32, 39, 52], here, we approximate the integra-

tion on Œ∏ via a Monte-Carlo (MC) sampling of ‚Ñ¶Œ∏. Let

ŒòL = {Œ∏l ‚àº œÉ(Œ∏)}Ll=1 denote a set of L parameters sam-

pled independently and uniformly from ‚Ñ¶Œ∏. We assume an

empirical reference measure,

¬µ0

=

1 M

M m=1

Œ¥(x

‚àí

x0m)

with M samples. The MC approximation is written as:

2

1L

GSW2,L(¬µi, ¬µj) = LM

œÜŒ∏l (¬µi) ‚àí œÜŒ∏l (¬µj )

2 2

.

(10)

l=1

Finally, our SWE embedding is calculated via:

œÜ(¬µi) = [ œÜ‚àöŒ∏1 (¬µi) ; ...; œÜ‚àöŒ∏L (¬µi) ] ‚àà RLM√ó1,

(11)

LM

LM

which satisÔ¨Åes:

œÜ(¬µi) ‚àí œÜ(¬µj) 2 = GSW2,L(¬µi, ¬µj) ‚âà GSW2(¬µi, ¬µj).
As for the approximation error, we rely on Theorem 6 in [44], which uses Ho¬®lder‚Äôs inequality and the results on the moments of the Monte Carlo estimation error to obtain:

E

2
|GSW2,L(¬µi, ¬µj ) ‚àí

GS

W

2 2

(¬µ,

ŒΩ

)|

‚â§

var(W22(¬µŒ∏i , ¬µŒ∏j )) L

(12)

The upper bo‚àöund indicates that the approximation error decreases with L. The numerator, however, is implicitly dependent on the dimensionality of input space. Meaning that a larger number of slices, L, is needed for higher dimensions.

Here we review the algorithmic steps to obtain SWE. We consider Xi = {xin}Nn=i‚àí01 as the input set with Ni elements, and X0 = {x0m}M m=‚àí01 denote the reference set of M samples where in general M = Ni. For a Ô¨Åxed slicer gŒ∏ we calculate {gŒ∏(xin)}Nn=i‚àí01 and {gŒ∏(x0m)}M m=‚àí01 and sort them increasingly. Let œÄi and œÄ0 denote the permutation indices (obtained from argsort). Also, let œÄ0‚àí1 denote the ordering
that permutes the sorted set back to the original ordering.
Then we numerically calculate the Monge coupling TiŒ∏ via:

TiŒ∏[m] = F¬µ‚àíŒ∏i1

œÄ0‚àí1[m] + 1 M

(13)

where F¬µŒ∏0 (x0m)

=

œÄ0‚àí1 [m]+1 M

,

assuming

that

the

indices

start from 0. Here F¬µ‚àíŒ∏i1 is calculated via interpolation. In

our experiments we used linear interpolation similar to [38].

Note that the dimensionality of the Monge coupling is only a function of the reference cardinality, i.e., TiŒ∏ ‚àà RM . Consequently, we write:

œÜŒ∏(Xi)[m] = (TiŒ∏[m] ‚àí gŒ∏(x0m))

(14)

and repeat this process for Œ∏ ‚àà {Œ∏l ‚àº œÉ(Œ∏)}Ll=1, while we emphasize that this process can be parallelized. The Ô¨Å-

nal embedding is achieved via weighting and concatenating

œÜŒ∏l s as in Eq.

(11), where the coefÔ¨Åcient

‚àö1 LM

allows us to

simplify the weighted Euclidean distance, ¬∑ ¬µ0,2, to Eu-

clidean distance, ¬∑ 2. Algorithm 1 summarizes the embed-

ding process, and Figure 1 provides a graphical depiction of

Algorithm 1 Sliced-Wasserstein Embedding
procedure SWE(Xi = {xin}Nn=i‚àí01, X0 = {x0m}M m=‚àí01, L) Generate a set of L samples ŒòL = {Œ∏l ‚àº U‚Ñ¶Œ∏ }Ll=1 Calculate gŒòL (X0) := {gŒ∏l (x0m)}m,l ‚àà RM√óL Calculate œÄ0 = argsort(gŒòL (X0)) and œÄ0‚àí1 (on m-
axis) Calculate gŒòL (Xi) := {gŒ∏l (xin)}n,l ‚àà RNi√óL Calculate œÄi = argsort(gŒòL (Xi)) (on n-axis) for l = 1 to L do Calculate the Monge coupling TiŒ∏l ‚àà RM (Eq.
(13))
end for Calculate the embedding œÜ(Xi) ‚àà RM√óL (Eq. (11)) return œÜ(Xi) end procedure

the process. Lastly, the SWE‚Äôs computational complexity for a set with cardinality |X| = N is O(LN (d + logN )), where we assumed the cardinality of the reference set is of the same order as N . Note that O(LN d) is the cost of slicing and O(LN log(N )) is the sorting and interpolation cost to calculate Eq. (13).

4.4. SLOSH

Our proposed Set LOcality Sensitive Hashing (SLOSH)

leverages the Sliced-Wasserstein Embedding (SWE) to embed training sets, Xtrain = {Xi|Xi = {xin ‚àà Rd}Nn=i‚àí01},
into a vector space where we can use Locality Sensitive

Hashing (LSH) to perform ANN search. We treat each input

set Xi

as a probability measure ¬µi(x)

=

1 Ni

Ni n=1

Œ¥(x

‚àí

xin). For a reference set X0 with cardinality |X0| = M

and L slices, we embed the input set Xi into a (RLM )-

dimensional vector space using Algorithm 1. With abuse

of notation we use œÜ(¬µi) and œÜ(Xi) interchangeably.

Given SWE, a family of (R, c, P1, P2)-sensitive LSH functions, H, will induce the following conditions,

‚Ä¢ If GSW2,L(Xi, Xj) ‚â§ R, then P r[h(œÜ(Xi)) = h(œÜ(Xj))] ‚â• P1, and

‚Ä¢ If GSW2,L(Xi, Xj) > cR, then P r[h(œÜ(Xi)) = h(œÜ(Xj))] ‚â§ P2
As mentioned in Section 2, for amplifying the gap between P1 and P2, one uses g(Xi) = [h1(œÜ(Xi)), ..., hk(œÜ(Xi))], which results in a code length, k, for each input set, Xi. Finally, if GSW2,L(Xi, Xj) ‚â§ R, by using T such codes, gt for t ‚àà {1, ..., T }, of length k, we can ensure collision at least in one of gts with probability 1 ‚àí (1 ‚àí P1k)T .

5. Experiments

We evaluated our set locality sensitive hashing via Sliced-Wasserstein Embedding algorithm against using Generalized Mean (GeM) pooling [53], Global Covariance (Cov) pooling [64], and Featurewise Sort Pooling (FSPool) [71] on various set-structured datasets. We note that while FSPool was proposed as a data-dependent embedding, here we devise its data-independent variation for fair comparison. Interstingly, FSPool can be thought as a special case of our SWE embedding where L = d and ŒòL is chosen as the identity matrix. To evaluate these methods, we tested all approaches on point cloud MNIST dataset (2D) [18, 34], ModelNet40 dataset (3D) [68], and the Oxford Buildings dataset [49] (i.e., Oxford 5k).

5.1. Baselines
Let Xi = {xin ‚àà Rd}Nn=i‚àí01 be the input set with Ni elements. We denote [Xi]k = {[xin]k}Nn=i‚àí01 as the set of all elements along the k‚Äôth dimension, k ‚àà {1, 2, ...d}. Below we provide a quick overview of the baseline approaches, which provide different set-2-vector mechanisms.
Generalized-Mean Pooling (GeM) [53] was originally proposed as a generalization of global mean and global max pooling on Convolutional Neural Network (CNN) features to boost image retrieval performance. Given the input Xi, GeM calculates the (p-th)-moment of each feature, f (p) ‚àà Rd, as:

[f (p)]k =

1

1 Ni

Ni
([xin ]k )p
n=1

p

(15)

When pooling parameter p = 1, we end up with average

pooling. While as p ‚Üí ‚àû, we get max pooling. In prac-

tice, we found that a concatenation of higher-order GeM features, i.e., œÜGeM (Xi) = [f (1); ...; f (p)] ‚àà Rpd, leads to the best performance, where p is GeM‚Äôs hyper-parameter.

Covariance Pooling [1, 64] presents another way to

capture second-order statistics and provide more informa-

tive representations. It was shown that this mechanism

can be applied to CNN features as an alternative to global

mean/max pooling to generate state-of-the-art results on fa-

cial expression recognition tasks [1]. Given input set Xi, the unbiased covariance matrix is computed by:

C

=

1 Ni ‚àí 1

Ni
(xin
n=1

‚àí ¬µi)(xin

‚àí ¬µi)T ,

(16)

where

¬µi

=

1 Ni

Ni n=1

xin.

The

output

matrix

can

be

further

regularized by adding a multiple of the trace to diagonal en-

tries of the covariance matrix to ensure symmetric positive

deÔ¨Åniteness (SPD), CŒª = C + Œªtrace(C)I where Œª is a

regularization hyper-parameter and I is the identity matrix.

Covariance pooling then uses œÜCov(Xi) = Ô¨Çatten(CŒª).

Table 1. Retrieval results of baselines and our approach on set retrieval on the three data sets.

GeM-1 (GAP) GeM-2 GeM-4 Cov FSPool SLOSH (L < d) SLOSH (L = d) SLOSH (L > d)

Point MNIST 2D

Precision@k/Accuracy

k=4

k=8

k=16

0.10/0.11 0.10/0.10 0.10/0.10

0.29/0.32 0.28/0.35 0.29/0.37

0.39/0.45 0.39/0.47 0.38/0.49

0.25/0.27 0.25/0.28 0.25/0.28

0.75/0.80 0.74/0.81 0.73/0.81

0.52/0.59 0.51/0.61 0.49/0.61

0.67/0.73 0.64/0.74 0.62/0.73

0.90/0.92 0.88/0.92 0.87/0.91

ModelNet 40

Precision@k/Accuracy

k=4

k=8

k=16

0.16/0.19 0.16/0.22 0.16/0.25

0.29/0.34 0.26/0.34 0.23/0.33

0.31/0.37 0.28/0.38 0.25/0.37

0.37/0.42 0.35/0.44 0.32/0.44

0.50/0.57 0.47/0.58 0.42/0.56

0.22/0.25 0.20/0.27 0.19/0.27

0.39/0.44 0.36/0.45 0.33/0.44

0.55/0.61 0.51/0.60 0.46/0.57

Oxford 5k

Precision@k/Accuracy

k=4

k=8

k=16

0.29/0.35 0.25/0.31 0.22/0.29

0.38/0.53 0.31/0.40 0.27/0.38

0.35/0.41 0.33/0.51 0.29/0.42

0.35/0.55 0.30/0.37 0.26/0.33

0.47/0.53 0.39/0.6 0.32/0.49

0.33/0.42 0.30/0.49 0.24/0.36

0.43/0.58 0.39/0.60 0.34/0.55

0.53/0.69 0.45/0.65 0.38/0.64

Featurewise Sort Pooling (FSPool) [71] is a powerful technique for learning representations from set-structured data. In short, this approach is based on sorting features along all elements of a set, [Xi]k:
f = [Sorted([Xi]1), ..., Sorted([Xi]d))] ‚àà RNi√ód (17)
The Ô¨Åxed-dimensional representation is then obtained via an interpolation along the Ni dimension of f . More precisely, a continuous linear operator W is learned and the inner product between this continuous operator and f is evaluated at M Ô¨Åxed points (i.e., leading to weighted summation over d), resulting in a M -dimensional embedding.
Given that we are interested in a data-independent set representation, we cannot rely on learning the continuous linear operator W . Instead, we perform interpolation along the Ni axis on M points and drop the inner product all together to obtain a (M √ó d)-dimensional data-independent set representation. The mentioned variation of FSPool is similar to the Sliced-Wasserstein Embedding when L = d and ŒòL = Id√ód, i.e., axis-aligned projections.
5.2. Datasets
Point Cloud MNIST 2D [18, 34] consists of 60,000 training samples and 10,000 testing samples. Each sample is a 2-dimensional point cloud derived from an image in the original MNIST dataset. The sets have various cardinalities in the range of |Xi| ‚àà [34, 351].
ModelNet40 [68] contains 3-dimensional point clouds converted from 12,311 CAD models in 40 common object categories. We used the ofÔ¨Åcial split with 9,843 samples for training and 2,468 samples for testing. We sample Ni points uniformly and randomly from the mesh faces of each object, where Ni = ni , ni ‚àº N (512, 64). To avoid any orientation bias, we randomly rotate the point clouds by œÅ ‚àà [0, 45] degrees around the x-axis. Finally, we normalize each sample to Ô¨Åt within the unit cube to avoid any scale bias and to enforce the methods to rely on shapes.
The Oxford Buildings Dataset [49] has 5,062 images containing eleven different Oxford landmarks. Each land-

Figure 2. Example sample retrievals.
mark has Ô¨Åve corresponding queries leading to 55 queries over which an object retrieval system can be evaluated. In our experiments, we use a pretrained VGG16 [57] on ImageNet1k [54] as a feature extractor, and use the features in the last convolutional layer as a set representation for an input image. We resize the images without cropping, which leads to varied input size images, and therefore gives set representations with varied cardinalities. We further perform a dimensionality reduction on these features to obtain sets of features in an (d = 8)-dimensional space.

k=4

k=8

k=16

k=4

k=8

k=16

Point MNIST

Point MNIST

ModelNet40

ModelNet40

Oxford 5k

Oxford 5k

Figure 3. Sensitivity to hash code length.
5.3. Results
For all datasets, we Ô¨Årst calculate the set-2-vector embeddings for all baselines and SWE. Then, we apply Locality-Sensitive Hashing (LSH) to the embedded sets and report Precision@k and accuracy (based on majority voting) for all the approaches on the test sets. We use the FAISS library [24], developed by Facebook AI Research, for LSH. For all methods, we use a hash code length of 1024, and we report our results for k = 4, 8, and 16. For SLOSH, we consider three different settings for the number of slices, namely, L < d, L = d, and L > d. We repeat the experiments Ô¨Åve times per method and report the mean Precision@k and accuracy in Table 1. In all experiments, the best hyper-parameters are selected for each approach based on cross validation. We see that SLOSH provides a consistent lead on all datasets, especially, when L > d. Figure 2 provide set retrieval examples on the three data sets. Next, we provide a sensitivity analysis of our approach.
Sensitivity to code length. For all datasets, we study the sensitivity of the different embeddings to the hashing codelength. We vary the code length from 16 to 1024, and report the average of Precision@k over Ô¨Åve runs. Figure 3 shows the outcome of this study. It can be seen that methods that encode higher statistical orders of the underlying distribution gain performance as a function of the hash code length. In addition, we see a strong performance gap between SWE and other set-2-vector approaches, which points at the more descriptive nature of our proposed method.
Sensitivity to the number of slices. Next, we study the sensitivity of SLOSH to the choice of number of slices, L, for the three datasets. We measure the average Precision@k over Ô¨Åve runs for various number of slices and for different

Figure 4. Sensitivity to number of slices.

k=4

k=8

k=16

Point MNIST

ModelNet40

Oxford 5k

Figure 5. Sensitivity to reference functions.
code-lengths and report our results in Figure 4. As expected we see a performance increase with respect to increasing L.
Sensitivity to the reference. Finally, we perform a study on the sensitivity to the choice of the reference set, X0. We measure the performance of SLOSH on the three datasets and for various code-lengths, when the reference set is: 1) calculated using K-Means on the elements of all sets, 2) a random set from the dataset, 3) sampled from a uniform distribution, and 4) sampled from the normal distribution. We see that SLOSH‚Äôs performance depends on the reference choice. However, we point out that our formulation implies that as L ‚Üí ‚àû the embedding becomes independent of the choice of the reference, and the observed sensitivity could be due to using Ô¨Ånite L.

6. Conclusion
We described a novel data-independent approach for Approximate Nearest Neighbor (ANN) search on setstructured data, with applications in set retrieval. We treat set elements as samples from an underlying distribution, and embed sets into a vector space in which the Euclidean distance approximates the Sliced-Wasserstein (SW) distance between the input distributions. We show that for a set X with cardinality |X| = N , our framework requires O(LN (d + Log(N ))) (sequential processing) or O(N (d + log(N ))) (parallel processing) calculations to obtain the embedding. We then use Locality Sensitive Hashing (LSH) for fast retrieval of nearest sets in our proposed embedding. We demonstrate a signiÔ¨Åcant boost over other data-independent approaches including Generalized Mean (GeM) on three different set retrieval tasks, namely, Point Cloud MNIST, ModelNet40, and the Oxford Buildings datasets. Finally, our proposed method is readily extendable to data-dependent settings by allowing optimization on the slices and the reference set.
7. Acknowledgements
This research was supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No. HR00112190132.
References
[1] Dinesh Acharya, Zhiwu Huang, Danda Pani Paudel, and Luc Van Gool. Covariance pooling for facial expression recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 367‚Äì374, 2018. 6
[2] David Alvarez Melis and Nicolo Fusi. Geometric dataset distances via optimal transport. Advances in Neural Information Processing Systems, 33, 2020. 3
[3] Alexandr Andoni and Piotr Indyk. Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions. In 2006 47th annual IEEE symposium on foundations of computer science (FOCS‚Äô06), pages 459‚Äì468. IEEE, 2006. 1, 2
[4] Alexandr Andoni, Piotr Indyk, Huy L Nguyen, and Ilya Razenshteyn. Beyond locality-sensitive hashing. In Proceedings of the twenty-Ô¨Åfth annual ACM-SIAM symposium on Discrete algorithms, pages 1018‚Äì1028. SIAM, 2014. 2
[5] Alexandr Andoni, Piotr Indyk, and Ilya Razenshteyn. Approximate nearest neighbor search in high dimensions. In Proceedings of the International Congress of Mathematicians: Rio de Janeiro 2018, pages 3287‚Äì3318. World ScientiÔ¨Åc, 2018. 1
[6] Alexandr Andoni and Ilya Razenshteyn. Optimal datadependent hashing for approximate near neighbors. In Proceedings of the forty-seventh annual ACM symposium on Theory of computing, pages 793‚Äì801, 2015. 1, 2

[7] Relja Arandjelovic and Andrew Zisserman. All about vlad. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 1578‚Äì1585, 2013. 2
[8] Martin Arjovsky, Soumith Chintala, and Le¬¥on Bottou. Wasserstein generative adversarial networks. In International conference on machine learning, pages 214‚Äì223. PMLR, 2017. 3
[9] Yogesh Balaji, Rama Chellappa, and Soheil Feizi. Normalized wasserstein for mixture distributions with applications in adversarial learning and domain adaptation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6500‚Äì6508, 2019. 3
[10] Jon Louis Bentley. Multidimensional binary search trees used for associative searching. Communications of the ACM, 18(9):509‚Äì517, 1975. 1
[11] Oren Boiman, Eli Shechtman, and Michal Irani. In defense of nearest-neighbor based image classiÔ¨Åcation. In 2008 IEEE Conference on Computer Vision and Pattern Recognition, pages 1‚Äì8. IEEE, 2008. 2
[12] Nicolas Bonneel, Julien Rabin, Gabriel Peyre¬¥, and Hanspeter PÔ¨Åster. Sliced and Radon Wasserstein barycenters of measures. Journal of Mathematical Imaging and Vision, 51(1):22‚Äì45, 2015. 5
[13] Nicolas Bonnotte. Unidimensional and evolution methods for optimal transportation. PhD thesis, Universite¬¥ Paris 11, France, 2013. 2, 3
[14] Moses S Charikar. Similarity estimation techniques from rounding algorithms. In Proceedings of the thiry-fourth annual ACM symposium on Theory of computing, pages 380‚Äì 388, 2002. 4
[15] Nicolas Courty, Re¬¥mi Flamary, Devis Tuia, and Alain Rakotomamonjy. Optimal transport for domain adaptation. IEEE transactions on pattern analysis and machine intelligence, 39(9):1853‚Äì1865, 2016. 3
[16] Bharath Bhushan Damodaran, Benjamin Kellenberger, Re¬¥mi Flamary, Devis Tuia, and Nicolas Courty. Deepjdot: Deep joint distribution optimal transport for unsupervised domain adaptation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 447‚Äì463, 2018. 3
[17] Mayur Datar, Nicole Immorlica, Piotr Indyk, and Vahab S Mirrokni. Locality-sensitive hashing scheme based on pstable distributions. In Proceedings of the twentieth annual symposium on Computational geometry, pages 253‚Äì 262, 2004. 1, 2, 3, 4
[18] Cristian Garcia. Point cloud mnist 2d, 2020. 6, 7
[19] Arthur Gretton, Karsten Borgwardt, Malte Rasch, Bernhard Scho¬®lkopf, and Alex Smola. A kernel method for the twosample-problem. Advances in neural information processing systems, 19:513‚Äì520, 2006. 2
[20] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of Wasserstein GANs. In Advances in Neural Information Processing Systems, pages 5767‚Äì5777, 2017. 3
[21] Piotr Indyk and Rajeev Motwani. Approximate nearest neighbors: towards removing the curse of dimensionality. In Proceedings of the thirtieth annual ACM symposium on Theory of computing, pages 604‚Äì613, 1998. 1, 2, 3, 4

[22] Tony Jebara, Risi Kondor, and Andrew Howard. Probability product kernels. The Journal of Machine Learning Research, 5:819‚Äì844, 2004. 2
[23] Herve¬¥ Je¬¥gou, Matthijs Douze, Cordelia Schmid, and Patrick Pe¬¥rez. Aggregating local descriptors into a compact image representation. In 2010 IEEE computer society conference on computer vision and pattern recognition, pages 3304‚Äì 3311. IEEE, 2010. 2
[24] Jeff Johnson, Matthijs Douze, and Herve¬¥ Je¬¥gou. Billionscale similarity search with gpus. arXiv preprint arXiv:1702.08734, 2017. 8
[25] Yannis Kalantidis, Clayton Mellina, and Simon Osindero. Cross-dimensional weighting for aggregated deep convolutional features. In European conference on computer vision, pages 685‚Äì701. Springer, 2016. 2
[26] Haim Kaplan and Jay Tenenbaum. Locality sensitive hashing for set-queries, motivated by group recommendations. In 17th Scandinavian Symposium and Workshops on Algorithm Theory (SWAT 2020). Schloss Dagstuhl-Leibniz-Zentrum fu¬®r Informatik, 2020. 3
[27] Soheil Kolouri, Navid Naderializadeh, Gustavo K. Rohde, and Heiko Hoffmann. Wasserstein embedding for graph learning. In International Conference on Learning Representations, 2021. 2, 3
[28] Soheil Kolouri, Kimia Nadjahi, Umut Simsekli, Roland Badeau, and Gustavo Rohde. Generalized sliced wasserstein distances. In Advances in Neural Information Processing Systems, pages 261‚Äì272, 2019. 3
[29] Soheil Kolouri, Se Rim Park, and Gustavo K. Rohde. The Radon cumulative distribution transform and its application to image classiÔ¨Åcation. Image Processing, IEEE Transactions on, 25(2):920‚Äì934, 2016. 4
[30] Soheil Kolouri, Se Rim Park, Matthew Thorpe, Dejan Slepcev, and Gustavo K Rohde. Optimal mass transport: Signal processing and machine-learning applications. IEEE Signal Processing Magazine, 34(4):43‚Äì59, 2017. 2, 3, 4
[31] Soheil Kolouri, Phillip E. Pope, Charles E. Martin, and Gustavo K. Rohde. Sliced Wasserstein auto-encoders. In International Conference on Learning Representations, 2019. 3, 5
[32] Soheil Kolouri, Yang Zou, and Gustavo K Rohde. SlicedWasserstein kernels for probability distributions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4876‚Äì4884, 2016. 2, 5
[33] Brian Kulis and Kristen Grauman. Kernelized localitysensitive hashing for scalable image search. In 2009 IEEE 12th international conference on computer vision, pages 2130‚Äì2137. IEEE, 2009. 2
[34] Yann LeCun, Le¬¥on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278‚Äì2324, 1998. 6, 7
[35] Chen-Yu Lee, Tanmay Batra, Mohammad Haris Baig, and Daniel Ulbricht. Sliced wasserstein discrepancy for unsupervised domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10285‚Äì10295, 2019. 3

[36] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye Teh. Set transformer: A framework for attention-based permutation-invariant neural networks. In International Conference on Machine Learning, pages 3744‚Äì3753. PMLR, 2019. 1, 2
[37] Alexander Levine and Soheil Feizi. Wasserstein smoothing: CertiÔ¨Åed robustness against wasserstein adversarial attacks. In International Conference on ArtiÔ¨Åcial Intelligence and Statistics, pages 3938‚Äì3947. PMLR, 2020. 3
[38] Antoine Liutkus, Umut Simsekli, Szymon Majewski, Alain Durmus, and Fabian-Robert Sto¬®ter. Sliced-wasserstein Ô¨Çows: Nonparametric generative modeling via optimal transport and diffusions. In International Conference on Machine Learning, pages 4104‚Äì4113. PMLR, 2019. 3, 5
[39] Antoine Liutkus, Umut S¬∏ ims¬∏ekli, Szymon Majewski, Alain Durmus, and Fabian-Robert Stoter. Sliced-Wasserstein Ô¨Çows: Nonparametric generative modeling via optimal transport and diffusions. In International Conference on Machine Learning, 2019. 5
[40] Gre¬¥goire Mialon, Dexiong Chen, Alexandre d‚ÄôAspremont, and Julien Mairal. A trainable optimal transport embedding for feature aggregation and its relationship to attention. In International Conference on Learning Representations, 2021. 3
[41] Caroline Moosmu¬®ller and Alexander Cloninger. Linear optimal transport embedding: Provable fast wasserstein distance computation and classiÔ¨Åcation for nonlinear problems. arXiv preprint arXiv:2008.09165, 2020. 4
[42] Krikamol Muandet, Kenji Fukumizu, Francesco Dinuzzo, and Bernhard Scho¬®lkopf. Learning from distributions via support measure machines. In Proceedings of the 25th International Conference on Neural Information Processing Systems-Volume 1, pages 10‚Äì18, 2012. 2
[43] Ryan L. Murphy, Balasubramaniam Srinivasan, Vinayak Rao, and Bruno Ribeiro. Janossy pooling: Learning deep permutation-invariant functions for variable-size inputs. In International Conference on Learning Representations, 2019. 2
[44] Kimia Nadjahi, Alain Durmus, Le¬¥na¬®ƒ±c Chizat, Soheil Kolouri, Shahin Shahrampour, and Umut S¬∏ ims¬∏ekli. Statistical and topological properties of sliced probability divergences. In Advances in Neural Information Processing Systems, 2020. 5
[45] Parth Nagarkar and K Selc¬∏uk Candan. Pslsh: An index structure for efÔ¨Åcient execution of set queries in high-dimensional spaces. In Proceedings of the 27th ACM International Conference on Information and Knowledge Management, pages 477‚Äì486, 2018. 3
[46] Se Rim Park, Soheil Kolouri, Shinjini Kundu, and Gustavo K Rohde. The cumulative distribution transform and linear pattern classiÔ¨Åcation. Applied and Computational Harmonic Analysis, 45(3):616‚Äì641, 2018. 4, 11
[47] Franc¬∏ois-Pierre Paty and Marco Cuturi. Subspace robust wasserstein distances. In International Conference on Machine Learning, 2019. 3
[48] Gabriel Peyre¬¥ and Marco Cuturi. Computational optimal transport. arXiv preprint arXiv:1803.00567, 2018. 2, 3

[49] James Philbin, Ondrej Chum, Michael Isard, Josef Sivic, and Andrew Zisserman. Object retrieval with large vocabularies and fast spatial matching. In 2007 IEEE conference on computer vision and pattern recognition, pages 1‚Äì8. IEEE, 2007. 6, 7
[50] Barnaba¬¥s Po¬¥czos and Jeff Schneider. Nonparametric estimation of conditional information and divergences. In ArtiÔ¨Åcial Intelligence and Statistics, pages 914‚Äì923. PMLR, 2012. 2
[51] Barnaba¬¥s Po¬¥czos, Liang Xiong, and Jeff Schneider. Nonparametric divergence estimation with applications to machine learning on distributions. In Proceedings of the Twenty-Seventh Conference on Uncertainty in ArtiÔ¨Åcial Intelligence, pages 599‚Äì608, 2011. 2
[52] Julien Rabin, Gabriel Peyre¬¥, Julie Delon, and Marc Bernot. Wasserstein barycenter and its application to texture mixing. In Scale Space and Variational Methods in Computer Vision, pages 435‚Äì446. Springer, 2012. 3, 5
[53] Filip Radenovic¬¥, Giorgos Tolias, and OndÀárej Chum. Finetuning cnn image retrieval with no human annotation. IEEE transactions on pattern analysis and machine intelligence, 41(7):1655‚Äì1668, 2018. 2, 6
[54] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211‚Äì252, 2015. 7
[55] Gregory Shakhnarovich, Trevor Darrell, and Piotr Indyk. Nearest-neighbor methods in learning and vision. IEEE Trans. Neural Networks, 19(2):377, 2008. 1
[56] Mohammad Shifat-E-Rabbi, Xuwang Yin, Abu Hasnat Mohammad Rubaiyat, Shiying Li, Soheil Kolouri, Akram Aldroubi, Jonathan M Nichols, and Gustavo K Rohde. Radon cumulative distribution transform subspace modeling for image classiÔ¨Åcation. Journal of Mathematical Imaging and Vision, 63(9):1185‚Äì1203, 2021. 4
[57] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. 7
[58] Aman Sinha, Hongseok Namkoong, and John Duchi. Certifying some distributional robustness with principled adversarial training. In International Conference on Learning Representations, 2018. 3
[59] Matteo Togninalli, Elisabetta Ghisu, Felipe Llinares-Lo¬¥pez, Bastian Rieck, and Karsten Borgwardt. Wasserstein weisfeiler-lehman graph kernels. Advances in Neural Information Processing Systems, 32:6439‚Äì6449, 2019. 3
[60] Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, and Bernhard Schoelkopf. Wasserstein auto-encoders. In International Conference on Learning Representations, 2018. 3
[61] Ce¬¥dric Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media, 2008. 2, 3
[62] Edward Wagstaff, Fabian Fuchs, Martin Engelcke, Ingmar Posner, and Michael A Osborne. On the limitations of representing functions on sets. In International Conference on Machine Learning, pages 6487‚Äì6494. PMLR, 2019. 1
[63] Ingo Wald and Vlastimil Havran. On building fast kd-trees for ray tracing, and on doing that in o (n log n). In 2006 IEEE

Symposium on Interactive Ray Tracing, pages 61‚Äì69. IEEE, 2006. 1 [64] Qilong Wang, Jiangtao Xie, Wangmeng Zuo, Lei Zhang, and Peihua Li. Deep cnns meet global covariance pooling: Better representation and generalization. IEEE transactions on pattern analysis and machine intelligence, 2020. 2, 6 [65] Wei Wang, Dejan SlepcÀáev, Saurav Basu, John A Ozolek, and Gustavo K Rohde. A linear optimal transportation framework for quantifying and visualizing variations in sets of images. International journal of computer vision, 101(2):254‚Äì 269, 2013. 4 [66] Eric Wong, Frank Schmidt, and Zico Kolter. Wasserstein adversarial examples via projected sinkhorn iterations. In International Conference on Machine Learning, pages 6808‚Äì 6817. PMLR, 2019. 3 [67] Kaiwen Wu, Allen Wang, and Yaoliang Yu. Stronger and faster wasserstein adversarial attacks. In International Conference on Machine Learning, pages 10377‚Äì10387. PMLR, 2020. 3 [68] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3d shapenets: A deep representation for volumetric shapes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1912‚Äì1920, 2015. 6, 7 [69] Liang Xiong and Jeff Schneider. Learning from point sets with observational bias. In Proceedings of the Thirtieth Conference on Uncertainty in ArtiÔ¨Åcial Intelligence, pages 898‚Äì 906, 2014. 2 [70] Manzil Zaheer, Satwik Kottur, Siamak Ravanbhakhsh, Barnaba¬¥s Po¬¥czos, Ruslan Salakhutdinov, and Alexander J Smola. Deep sets. In Proceedings of the 31st International Conference on Neural Information Processing Systems, pages 3394‚Äì3404, 2017. 1, 2 [71] Yan Zhang, Jonathon Hare, and Adam Pru¬®gel-Bennett. Fspool: Learning set representations with featurewise sort pooling. arXiv preprint arXiv:1906.02795, 2019. 6, 7 [72] Yan Zhang, Jonathon Hare, and Adam Pru¬®gel-Bennett. Fspool: Learning set representations with featurewise sort pooling. In International Conference on Learning Representations, 2020. 2
8. Supplementary Materials
8.1. Proofs
Here we include the proof for the C1 and C2 conditions covered in Section 4. Recall that ¬µi represents a probability measure, ¬µŒ∏i represents gŒ∏#¬µi, where gŒ∏ : Rd ‚Üí R with some regularity constraints, and that we deÔ¨Åne the cumulative distribution transform (CDT) [46] of ¬µŒ∏i as
œÜŒ∏(¬µi) := (TiŒ∏ ‚àí id),
where TiŒ∏ is the Monge map/coupling, and id denotes the identity function. For a Ô¨Åxed Œ∏, here we prove that œÜŒ∏(¬µi) satisÔ¨Åes the following conditions:

Method Gem-p Cov. FSPool SLOSH

Complexity O(N p2d) O(N d2)
O(N dlogN )
O(LN (d + logN ))

Table 2. Computational complexities.

C1. The weighted 2-norm of the embedded slice, œÜŒ∏(¬µi), satisÔ¨Åes:

œÜŒ∏(¬µi) ¬µŒ∏0,2 =

1 2
œÜŒ∏(¬µi(t)) 22d¬µŒ∏0(t)
R

= W2(¬µŒ∏i , ¬µŒ∏0),

Proof. We start by writing the squared distance:

As a corollary of C1 and C2 we have:

G

S

W

2 2

(¬µi,

¬µj

)

=

W22(¬µŒ∏i , ¬µŒ∏j )dœÉ(Œ∏) =

‚Ñ¶Œ∏

‚Ñ¶Œ∏

œÜŒ∏(¬µi) ‚àí œÜŒ∏(¬µj)

2 ¬µŒ∏0

,2

dœÉ

(Œ∏)

8.2. Computational complexities

For the sake of completeness, here we include the com-
putational complexities of the baseline methods used in our
paper. In Table 2, we provide the computational complexity of embedding a set X = {xn ‚àà Rd}Nn=1 into a vector space.

œÜŒ∏ (¬µi )

2 ¬µŒ∏0 ,2

=

œÜŒ∏ (¬µi (t))

2 2

d¬µŒ∏0

(t)

R

=

TiŒ∏(t) ‚àí t

2 2

d¬µŒ∏0

(t)

R

=

R

(F¬µ‚àíŒ∏i1 ‚ó¶ F¬µŒ∏0 )(t) ‚àí t

2 2

d¬µŒ∏0

(t)

1

=
0

F¬µ‚àíŒ∏i1(œÑ ) ‚àí F¬µ‚àíŒ∏01(œÑ ) 22dœÑ

= W22(¬µŒ∏i , ¬µŒ∏0)

where we used the deÔ¨Ånition of the one-dimensional Monge
map, TiŒ∏ = F¬µ‚àíŒ∏i1 ‚ó¶ F¬µŒ∏0 , and the change of variable œÑ =
F¬µŒ∏0 (t). The corollary, œÜŒ∏(¬µ0) ¬µŒ∏0,2 = 0, is trivial as W22(¬µŒ∏0, ¬µŒ∏0) = 0.

C2. The weighted 2 distance satisÔ¨Åes:

œÜŒ∏(¬µi) ‚àí œÜŒ∏(¬µj ) ¬µŒ∏0,2 = W2(¬µŒ∏i , ¬µŒ∏j ).

(18)

Proof. Similar to the previous proof:

œÜŒ∏(¬µi) ‚àí œÜŒ∏(¬µj)

2 ¬µŒ∏0 ,2

=

œÜŒ∏(¬µi(t)) ‚àí œÜŒ∏(¬µj(t)) 22d¬µŒ∏0(t)

R

= TiŒ∏(t) ‚àí TjŒ∏(t) 22d¬µŒ∏0(t)
R

= R (F¬µ‚àíŒ∏i1 ‚ó¶ F¬µŒ∏0 )(t) ‚àí (F¬µ‚àíŒ∏j1 ‚ó¶ F¬µŒ∏0 )(t) 22d¬µŒ∏0(t)

1

=
0

F¬µ‚àíŒ∏i1(œÑ ) ‚àí F¬µ‚àíŒ∏j1(œÑ )

2 2

dœÑ

= W22(¬µŒ∏i , ¬µŒ∏j )

where again we used the deÔ¨Ånition of the one-dimensional Monge map, TiŒ∏ = F¬µ‚àíŒ∏i1 ‚ó¶ F¬µŒ∏0 , and the change of variable œÑ = F¬µŒ∏0 (t).

